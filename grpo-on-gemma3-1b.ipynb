{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V6E1"},"accelerator":"TPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7045423,"sourceType":"datasetVersion","datasetId":4054119,"isSourceIdPinned":false}],"dockerImageVersionId":31155,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GRPO Demo\n\nThis is a way of training the [Gemma](https://deepmind.google/models/gemma/)\n3 1B-IT model on the [GSM8K math reasoning benchmark](https://huggingface.co/datasets/openai/gsm8k)\nusing [Group Relative Policy Optimization (GRPO)](https://arxiv.org/pdf/2402.03300).\nGRPO can enhance your model's problem-solving skills on mathematical word problems,\ncoding problems, etc.\n\nThis notebook uses a `v5e-8` TPU for Gemma3-1b-it. ","metadata":{"id":"abdhOBYHqYz6"}},{"cell_type":"code","source":"# Check for accelerators \n\ndef is_tpu():\n    try:\n        import torch_xla.core.xla_model as xm\n        return xm.xrt_world_size() > 0\n    except ImportError:\n        return False\n\ndef is_gpu():\n    try:\n        import torch\n        return torch.cuda.is_available() and not is_tpu()\n    except ImportError:\n        return False\n\ndef is_cpu():\n    return not is_gpu() and not is_tpu()\n\ndef installs_and_imports():\n    import os\n    os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n    if is_tpu():\n        print(\"Running on TPU\")\n        !pip install -q kagglehub\n\n        !pip install -q ipywidgets\n        \n        !pip install -q tensorflow\n        !pip install -q tensorflow_datasets\n        !pip install -q tensorboardX\n        !pip install -q transformers\n        !pip install -q grain\n        !pip install \"google-tunix[prod]==0.1.3\"\n        \n        # !pip install -q git+https://github.com/google/tunix\n        # !pip install -q git+https://github.com/google/qwix\n        \n        !pip uninstall -q -y flax\n        # !pip install -U flax\n        !pip install flax==0.12.0\n        !pip install -q datasets wandb==0.22.0\n\n        from tunix.generate import sampler as sampler_lib\n        from tunix.generate import tokenizer_adapter as tokenizer_lib\n    elif is_gpu() is not False:\n        print(\"Running on GPU\")\n        !pip install -q kagglehub ipywidgets tensorflow tensorflow_datasets tensorboardX transformers\n        !pip install -q grain\n        !pip install \"google-tunix[prod]==0.1.3\"\n        !pip uninstall -q -y flax\n        !pip install flax==0.12.0\n        !pip install -q datasets wandb==0.22.0\n        # GPU JAX wheels (replace XXX with your CUDA version)\n        !pip install --upgrade jax jaxlib==0.4.26+cudaXXX -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n    else:\n        print(\"Running on CPU, nothing will work :(\")\n        !pip install -q kagglehub ipywidgets tensorflow tensorflow_datasets tensorboardX transformers\n        !pip install -q grain\n        !pip install \"google-tunix[prod]==0.1.3\"\n        !pip uninstall -q -y flax\n        !pip install flax==0.12.0\n        !pip install -q datasets wandb==0.22.0\n        !pip install jax jaxlib\n\n\ninstalls_and_imports()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:20:00.101809Z","iopub.execute_input":"2025-11-28T16:20:00.102061Z","iopub.status.idle":"2025-11-28T16:21:12.025466Z","shell.execute_reply.started":"2025-11-28T16:20:00.102041Z","shell.execute_reply":"2025-11-28T16:21:12.024585Z"}},"outputs":[{"name":"stdout","text":"Running on GPU\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.9/319.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\ngoogle-cloud-bigtable 2.32.0 requires google-api-core[grpc]<3.0.0,>=2.17.0, but you have google-api-core 1.34.1 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.4/506.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting google-tunix==0.1.3 (from google-tunix[prod]==0.1.3)\n  Downloading google_tunix-0.1.3-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.1.1)\nCollecting flax>=0.11.1 (from google-tunix==0.1.3->google-tunix[prod]==0.1.3)\n  Downloading flax-0.12.1-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: gcsfs in /usr/local/lib/python3.11/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.3.0)\nRequirement already satisfied: grain in /usr/local/lib/python3.11/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.2.15)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.36.0)\nCollecting jaxtyping (from google-tunix==0.1.3->google-tunix[prod]==0.1.3)\n  Downloading jaxtyping-0.3.3-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.1.6)\nRequirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.3.13)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.60.0)\nRequirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.3.0)\nRequirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.1.1)\nCollecting qwix (from google-tunix==0.1.3->google-tunix[prod]==0.1.3)\n  Downloading qwix-0.1.4-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.2.0)\nRequirement already satisfied: tensorboardX in /usr/local/lib/python3.11/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.6.4)\nRequirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.11/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.9.9)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.67.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.53.3)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.9)\nCollecting jax!=0.7.2,>=0.6.0 (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3)\n  Downloading jax-0.8.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.26.4)\nRequirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.1.1)\nRequirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.2.5)\nRequirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.11.19)\nRequirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.74)\nRequirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (14.1.0)\nRequirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.15.0)\nRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.0.3)\nRequirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.9)\nCollecting jaxlib<=0.8.1,>=0.8.1 (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3)\n  Downloading jaxlib-0.8.1-cp311-cp311-manylinux_2_27_x86_64.whl.metadata (1.3 kB)\nCollecting ml_dtypes>=0.5.0 (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3)\n  Downloading ml_dtypes-0.5.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\nCollecting numpy>=1.23.2 (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3)\n  Downloading numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (3.4.0)\nRequirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.11/dist-packages (from jax!=0.7.2,>=0.6.0->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (1.15.3)\nCollecting libtpu==0.0.30.* (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3)\n  Downloading libtpu-0.0.30-cp311-cp311-manylinux_2_31_x86_64.whl.metadata (1.2 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (2.32.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.19.1)\nCollecting pyarrow>=21.0.0 (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.70.16)\nRequirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.9.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (25.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.1.10)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.12.15)\nRequirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.4.2)\nCollecting fsspec<=2025.9.0,>=2023.1.0 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3)\n  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.11/dist-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.40.3)\nRequirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.11/dist-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.2.2)\nRequirement already satisfied: google-cloud-storage in /usr/local/lib/python3.11/dist-packages (from gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.19.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.4.0)\nRequirement already satisfied: array-record>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.8.3)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.1.1)\nRequirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.11/dist-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.13.0)\nRequirement already satisfied: protobuf>=5.28.3 in /usr/local/lib/python3.11/dist-packages (from grain->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (5.29.5)\nCollecting wadler-lindig>=0.1.3 (from jaxtyping->google-tunix==0.1.3->google-tunix[prod]==0.1.3)\n  Downloading wadler_lindig-0.1.7-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.0.2)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.43.0)\nCollecting numpy>=1.23.2 (from flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3)\n  Downloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.9.3)\nRequirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.9)\nRequirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.2.1)\nRequirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (7.1.0)\nRequirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.7)\nRequirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.17.2)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.1.0)\nRequirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.10.2)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.17.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.9.18)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.5.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.20.1)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.8.1)\nRequirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (6.5.2)\nRequirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.23.0)\nINFO: pip is looking at multiple versions of fsspec[http] to determine which version is compatible with other requirements. This could take a while.\nCollecting fsspec[http]<=2025.9.0,>=2023.1.0 (from datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3)\n  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n  Downloading fsspec-2025.5.0-py3-none-any.whl.metadata (11 kB)\n  Downloading fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (5.5.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.4.2)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.9.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->jax[tpu]!=0.7.2,>=0.6.0; extra == \"prod\"->google-tunix[prod]==0.1.3) (2025.8.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.19.2)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.0.0)\nCollecting google-api-core<3.0.0dev,>=2.15.0 (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3)\n  Downloading google_api_core-2.28.1-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.4.3)\nRequirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.7.2)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.7.1)\nRequirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.90)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.6.0)\nRequirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (4.12.3)\nRequirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.20.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (2025.2)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from promise->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.17.0)\nRequirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.17.0)\nRequirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow_datasets->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.70.0)\nRequirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.87->optax->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.0.0)\nRequirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (1.26.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.11.1->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.1.2)\nRequirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (0.6.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs->google-tunix==0.1.3->google-tunix[prod]==0.1.3) (3.3.1)\nDownloading google_tunix-0.1.3-py3-none-any.whl (253 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.7/253.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading flax-0.12.1-py3-none-any.whl (488 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.2/488.2 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jax-0.8.1-py3-none-any.whl (2.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading libtpu-0.0.30-cp311-cp311-manylinux_2_31_x86_64.whl (192.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.7/192.7 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jaxtyping-0.3.3-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading qwix-0.1.4-py3-none-any.whl (91 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.6/91.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jaxlib-0.8.1-cp311-cp311-manylinux_2_27_x86_64.whl (80.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ml_dtypes-0.5.4-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading wadler_lindig-0.1.7-py3-none-any.whl (20 kB)\nDownloading google_api_core-2.28.1-py3-none-any.whl (173 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.7/173.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: wadler-lindig, pyarrow, numpy, libtpu, fsspec, ml_dtypes, jaxtyping, jaxlib, google-api-core, jax, flax, qwix, google-tunix\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.9.0\n    Uninstalling fsspec-2025.9.0:\n      Successfully uninstalled fsspec-2025.9.0\n  Attempting uninstall: ml_dtypes\n    Found existing installation: ml-dtypes 0.4.1\n    Uninstalling ml-dtypes-0.4.1:\n      Successfully uninstalled ml-dtypes-0.4.1\n  Attempting uninstall: jaxlib\n    Found existing installation: jaxlib 0.5.1\n    Uninstalling jaxlib-0.5.1:\n      Successfully uninstalled jaxlib-0.5.1\n  Attempting uninstall: google-api-core\n    Found existing installation: google-api-core 1.34.1\n    Uninstalling google-api-core-1.34.1:\n      Successfully uninstalled google-api-core-1.34.1\n  Attempting uninstall: jax\n    Found existing installation: jax 0.5.2\n    Uninstalling jax-0.5.2:\n      Successfully uninstalled jax-0.5.2\n  Attempting uninstall: flax\n    Found existing installation: flax 0.10.6\n    Uninstalling flax-0.10.6:\n      Successfully uninstalled flax-0.10.6\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.0.2 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.0.2 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.0.2 which is incompatible.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.28.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.5 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires ml-dtypes<0.5.0,>=0.4.0, but you have ml-dtypes 0.5.4 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed flax-0.12.1 fsspec-2025.3.0 google-api-core-2.28.1 google-tunix-0.1.3 jax-0.8.1 jaxlib-0.8.1 jaxtyping-0.3.3 libtpu-0.0.30 ml_dtypes-0.5.4 numpy-2.0.2 pyarrow-22.0.0 qwix-0.1.4 wadler-lindig-0.1.7\nCollecting flax==0.12.0\n  Downloading flax-0.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from flax==0.12.0) (2.0.2)\nRequirement already satisfied: jax>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from flax==0.12.0) (0.8.1)\nRequirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax==0.12.0) (1.1.1)\nRequirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax==0.12.0) (0.2.5)\nRequirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax==0.12.0) (0.11.19)\nRequirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax==0.12.0) (0.1.74)\nRequirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax==0.12.0) (14.1.0)\nRequirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from flax==0.12.0) (4.15.0)\nRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax==0.12.0) (6.0.3)\nRequirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax==0.12.0) (0.1.9)\nRequirement already satisfied: jaxlib<=0.8.1,>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.7.1->flax==0.12.0) (0.8.1)\nRequirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.7.1->flax==0.12.0) (0.5.4)\nRequirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.7.1->flax==0.12.0) (3.4.0)\nRequirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.11/dist-packages (from jax>=0.7.1->flax==0.12.0) (1.15.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax==0.12.0) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax==0.12.0) (2.19.2)\nRequirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from optax->flax==0.12.0) (1.4.0)\nRequirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax->flax==0.12.0) (0.1.90)\nRequirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax==0.12.0) (1.13.0)\nRequirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax==0.12.0) (1.6.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax==0.12.0) (5.29.5)\nRequirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax==0.12.0) (4.12.3)\nRequirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax==0.12.0) (3.20.1)\nRequirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.87->optax->flax==0.12.0) (1.0.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax==0.12.0) (0.1.2)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.12.0) (2025.3.0)\nRequirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.12.0) (6.5.2)\nRequirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.12.0) (3.23.0)\nDownloading flax-0.12.0-py3-none-any.whl (466 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: flax\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed flax-0.12.0\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.6/19.6 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hLooking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\nRequirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (0.8.1)\n\u001b[31mERROR: Ignored the following yanked versions: 0.4.32\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement jaxlib==0.4.26+cudaXXX (from versions: 0.3.24+cuda11.cudnn805, 0.3.24+cuda11.cudnn82, 0.3.25+cuda11.cudnn805, 0.3.25+cuda11.cudnn82, 0.4.1+cuda11.cudnn82, 0.4.1+cuda11.cudnn86, 0.4.2+cuda11.cudnn82, 0.4.2+cuda11.cudnn86, 0.4.3+cuda11.cudnn82, 0.4.3+cuda11.cudnn86, 0.4.4+cuda11.cudnn82, 0.4.4+cuda11.cudnn86, 0.4.6, 0.4.6+cuda11.cudnn82, 0.4.6+cuda11.cudnn86, 0.4.7, 0.4.7+cuda11.cudnn82, 0.4.7+cuda11.cudnn86, 0.4.7+cuda12.cudnn88, 0.4.9, 0.4.9+cuda11.cudnn86, 0.4.9+cuda12.cudnn88, 0.4.10, 0.4.10+cuda11.cudnn86, 0.4.10+cuda12.cudnn88, 0.4.11, 0.4.11+cuda11.cudnn86, 0.4.11+cuda12.cudnn88, 0.4.12, 0.4.12+cuda11.cudnn86, 0.4.12+cuda12.cudnn89, 0.4.13, 0.4.13+cuda11.cudnn86, 0.4.13+cuda12.cudnn89, 0.4.14, 0.4.14+cuda11.cudnn86, 0.4.14+cuda12.cudnn89, 0.4.16, 0.4.16+cuda11.cudnn86, 0.4.16+cuda12.cudnn89, 0.4.17, 0.4.17+cuda11.cudnn86, 0.4.17+cuda12.cudnn89, 0.4.18, 0.4.18+cuda11.cudnn86, 0.4.18+cuda12.cudnn89, 0.4.19, 0.4.19+cuda11.cudnn86, 0.4.19+cuda12.cudnn89, 0.4.20, 0.4.20+cuda11.cudnn86, 0.4.20+cuda12.cudnn89, 0.4.21, 0.4.21+cuda11.cudnn86, 0.4.21+cuda12.cudnn89, 0.4.22, 0.4.22+cuda11.cudnn86, 0.4.22+cuda12.cudnn89, 0.4.23, 0.4.23+cuda11.cudnn86, 0.4.23+cuda12.cudnn89, 0.4.24, 0.4.24+cuda11.cudnn86, 0.4.24+cuda12.cudnn89, 0.4.25, 0.4.25+cuda11.cudnn86, 0.4.25+cuda12.cudnn89, 0.4.26, 0.4.26+cuda12.cudnn89, 0.4.27, 0.4.27+cuda12.cudnn89, 0.4.28, 0.4.28+cuda12.cudnn89, 0.4.29, 0.4.29+cuda12.cudnn91, 0.4.30, 0.4.31, 0.4.33, 0.4.34, 0.4.35, 0.4.36, 0.4.38, 0.5.0, 0.5.1, 0.5.3, 0.6.0, 0.6.1, 0.6.2, 0.7.0, 0.7.1, 0.7.2, 0.8.0, 0.8.1)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for jaxlib==0.4.26+cudaXXX\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Install necessary libraries","metadata":{"id":"afofSj37qYz6"}},{"cell_type":"code","source":"import wandb, os\nfrom kaggle_secrets import UserSecretsClient\nos.environ['WANDB_API_KEY'] = UserSecretsClient().get_secret(\"WANDB_API_KEY\")","metadata":{"id":"953CnpN5xQc_","outputId":"28f62af4-2d67-4034-dc33-d0f69ddf88cc","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:22:30.444551Z","iopub.execute_input":"2025-11-28T16:22:30.445753Z","iopub.status.idle":"2025-11-28T16:22:32.263449Z","shell.execute_reply.started":"2025-11-28T16:22:30.445717Z","shell.execute_reply":"2025-11-28T16:22:32.262827Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"LnF9ZACiTn1k"}},{"cell_type":"code","source":"import functools\nimport gc\nimport os\nfrom pprint import pprint\nimport re\n\nimport csv\nimport shutil\n\n#When using the version flax==0.12.0, this import works only if TPU is enabled\n#https://flax.readthedocs.io/en/latest/nnx_basics.html\n#from flax import nnx\nimport grain\nimport humanize\nimport jax\nimport jax.numpy as jnp\nimport kagglehub\nimport optax\nfrom orbax import checkpoint as ocp\nfrom pathlib import Path\nimport qwix\nimport tensorflow_datasets as tfds\nfrom tqdm.auto import tqdm\n\n# from tunix.models.gemma3 import model as gemma_lib\n# from tunix.models.gemma3 import params as params_lib\nfrom tunix.models.gemma3 import params\nfrom tunix.models.gemma3 import model\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.sft import metrics_logger\nfrom datasets import load_dataset\nfrom google import genai\nfrom pydantic import BaseModel, Field\nfrom typing import List, Optional","metadata":{"id":"McTNo_r8Tn1k","outputId":"67f038b8-509b-46ed-b027-9b72ed7b628c","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:51:07.989433Z","iopub.execute_input":"2025-11-28T16:51:07.990093Z","iopub.status.idle":"2025-11-28T16:51:10.789939Z","shell.execute_reply.started":"2025-11-28T16:51:07.990069Z","shell.execute_reply":"2025-11-28T16:51:10.789353Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"## Configuration of Hyperparameters\n\nThe choice of the hyperparameters is tricky because I cant use gridsearch or optuna to optimize them, it would be very expensive. The model size is small, so I though maybe to use a smaller rank, on the other hand the task we mean to train is a big change of policy, so I will keep the rank at 32","metadata":{"id":"Eu_NI9nHTn1k"}},{"cell_type":"code","source":"class Configs:\n    # ====== Data ======\n    TRAIN_DATA_DIR = \"./data/train\"\n    TEST_DATA_DIR = \"./data/test\"\n    TRAIN_FRACTION = 0.8\n    \n    # ====== LoRA ======\n    RANK = 32\n    ALPHA = 32.0\n    \n    # ====== Sharding ======\n    MESH = [(1, 4), (\"fsdp\", \"tp\")]\n    \n    # ====== GRPO ======\n    # === Generation during GRPO training ===\n    MAX_PROMPT_LENGTH = 256\n    TOTAL_GENERATION_STEPS = 512\n    \n    # Important to keep a high-ish temperature for varied, diverse responses during\n    # training.\n    TEMPERATURE = 0.9\n    TOP_P = 0.95\n    TOP_K = 50\n\n    # The number of times the policy generates multiple responses for a given prompt within a single training step. \n    # So basically the number of groups, I will set it to 16. Originally google kept it as 4, but seems to litle for such a task\n    NUM_GENERATIONS = 16\n    \n    # === other GRPO configs ===\n    # The number of iterations per batch (𝜇 in GRPO algo 1).\n    NUM_ITERATIONS = 1\n    # The coefficient for the KL divergence penalty (𝛽) in the GRPO loss function.\n    # Important to keep a high enough value for this, otherwise, the KL divergence\n    # can increase unchecked and the model may overfit or suffer from catastrophical forgetting.\n    # But using LoRa is that really a risk ? we wont even touch the the original matrix/policy..... lets keep it lower, say 0.04 (they had 0.08)\n    BETA = 0.04\n    \n    # Epsilon value for clipping. Similar to PPO, for\n    # stable updates.\n    EPSILON = 0.2\n    \n    # ====== Training ======\n    TRAIN_MICRO_BATCH_SIZE = 4\n    # Increase `NUM_BATCHES` and `MAX_STEPS` for better results.\n    NUM_BATCHES = 3738\n    # Keep `NUM_TEST_BATCHES` low so that evaluation runs quickly. It can be\n    # increased to a max. of 330 (if batch size is 4).\n    NUM_TEST_BATCHES = 100\n    \n    EVAL_EVERY_N_STEPS = 10  # this doesn't matter if `TRAIN_FRACTION = 1.0`.\n    NUM_EPOCHS = 2  # each epoch takes around 1.5 hours\n    \n    # Number of training steps.\n    MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n    \n    # === AdamW, warmup, cosine scheduler ===\n    LEARNING_RATE = 3e-6\n    B1 = 0.9\n    B2 = 0.99\n    WEIGHT_DECAY = 0.1\n    # == Cosine decay with warmup scheduler ==\n    # Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n    # steps, and then gradually decrease the learning rate to 0 using cosine\n    # scheduler.\n    WARMUP_STEPS = 0.1 * MAX_STEPS\n    # == Grad clipping ==\n    # Grad clipping to prevent large gradients. Found this\n    # important to keep KL divergence in check.\n    MAX_GRAD_NORM = 0.1\n    \n    # Checkpoint saving\n    INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n    CKPT_DIR = \"/tmp/content/ckpts/\"\n    SAVE_INTERVAL_STEPS = 500\n    MAX_TO_KEEP = 4\n\n    # ====== Inference ======\n    GENERATION_CONFIGS = {\n        # greedy search\n        \"greedy\": {\"temperature\": 1e-3, \"top_k\": 3, \"top_p\": 1.0},\n        # some randomness\n        \"standard\": {\"temperature\": 0.8, \"top_k\": 50, \"top_p\": 0.95},\n        # liberal\n        \"liberal\": {\"temperature\": 0.95, \"top_k\": 2000, \"top_p\": 1.0},\n    }\n\n    # Prompt stuff\n    #let's define some special tokens. We instruct the model to first reason\n    #between the `<reasoning>` and `</reasoning>` tokens. After\n    #reasoning, we expect it to provide the answer between the `<answer>` and\n    #`</answer>` tokens.\n    reasoning_start = \"<reasoning>\"\n    reasoning_end = \"</reasoning>\"\n    solution_start = \"<answer>\"\n    solution_end = \"</answer>\"\n    \n    \n    SYSTEM_PROMPT = f\"\"\"You are given a problem. Think about the problem and \\\n    provide your reasoning. Place it between {reasoning_start} and \\\n    {reasoning_end}. Then, provide the final answer (i.e., just one numerical \\\n    value) between {solution_start} and {solution_end}.\"\"\"\n    \n    TEMPLATE = \"\"\"<start_of_turn>user\n    {system_prompt}\n    \n    {question}<end_of_turn>\n    <start_of_turn>model\"\"\"","metadata":{"id":"ZPPKme47Tn1k","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:23:25.625820Z","iopub.execute_input":"2025-11-28T16:23:25.626195Z","iopub.status.idle":"2025-11-28T16:23:25.633725Z","shell.execute_reply.started":"2025-11-28T16:23:25.626166Z","shell.execute_reply":"2025-11-28T16:23:25.632985Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Utility functions","metadata":{"id":"ngjtE-63Tn1k"}},{"cell_type":"code","source":"def show_hbm_usage():\n  \"\"\"Displays memory usage per device.\"\"\"\n  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n\n  for d in jax.local_devices():\n    stats = d.memory_stats()\n    used = stats[\"bytes_in_use\"]\n    limit = stats[\"bytes_limit\"]\n    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")","metadata":{"id":"wjMFOr7aTn1k","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:23:26.461189Z","iopub.execute_input":"2025-11-28T16:23:26.461855Z","iopub.status.idle":"2025-11-28T16:23:26.466063Z","shell.execute_reply.started":"2025-11-28T16:23:26.461830Z","shell.execute_reply":"2025-11-28T16:23:26.465256Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Data preprocessing","metadata":{"id":"6BtpYMlaTn1k"}},{"cell_type":"markdown","source":"#### I use OpenAI's [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k), which comprises grade school math word problems.","metadata":{"id":"WASP9N5JTn1k"}},{"cell_type":"code","source":"def extract_hash_answer(text: str) -> str | None:\n  if \"####\" not in text:\n    return None\n  return text.split(\"####\")[1].strip()\n\n\ndef _load_from_tfds(data_dir: str, split: str):\n  import tensorflow_datasets.text.gsm8k\n  return tfds.data_source(\n      \"gsm8k\",\n      split=split,\n      data_dir=data_dir,\n      builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n      download=True,\n  )\n\n\ndef download_kaggle_dataset(target_dir=\"./data/gsm8k\"):\n  os.makedirs(target_dir, exist_ok=True)\n  src = kagglehub.dataset_download(\"thedevastator/grade-school-math-8k-q-a\")\n  src = Path(src)\n  dst = Path(target_dir)\n\n  for csv_file in src.glob(\"*.csv\"):  # match all CSV files\n    shutil.copy2(csv_file, dst / csv_file.name)\n    print(f\"Copied {csv_file.name} → {dst/csv_file.name}\")\n  return target_dir\n\n\ndef get_dataset(data_dir, split=\"train\", source=\"tfds\") -> grain.MapDataset:\n  # Download data\n  if not os.path.exists(data_dir):\n    os.makedirs(data_dir)\n\n  if source == \"tfds\":\n    import tensorflow_datasets.text.gsm8k\n    data = tfds.data_source(\n        \"gsm8k\",\n        split=split,\n        data_dir=data_dir,\n        builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n        download=True,\n    )\n\n  elif source == \"kaggle\":\n    kaggle_dir = download_kaggle_dataset(data_dir)\n    file_name = \"main_\" + split + \".csv\"\n    csv_path = os.path.join(kaggle_dir, file_name)  # adjust filename if needed\n\n    data = []\n    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n      reader = csv.DictReader(csvfile)\n      for row in reader:\n        data.append({\n            \"question\": row[\"question\"],\n            \"answer\": row[\"answer\"],\n        })\n\n  elif source == \"huggingface\":    \n    os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n    data = load_dataset(\"gsm8k\", \"main\", split=split)\n      \n  else:\n    raise ValueError(f\"Unknown source: {source}\")\n\n  def _as_text(v):\n    return v if isinstance(v, str) else v.decode(\"utf-8\")\n\n  dataset = (\n      grain.MapDataset.source(data)\n      .shuffle(seed=42)\n      .map(\n          lambda x: {\n              # passed to model forward pass\n              \"prompts\": Configs.TEMPLATE.format(\n                  system_prompt=Configs.SYSTEM_PROMPT,\n                  question=_as_text(x[\"question\"]),\n              ),\n              # passed to reward functions\n              \"question\": _as_text(x[\"question\"]),\n              # passed to reward functions\n              \"answer\": extract_hash_answer(_as_text(x[\"answer\"])),\n          }\n      )\n  )\n  return dataset","metadata":{"id":"gTGjcSMNTn1k","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:23:28.038963Z","iopub.execute_input":"2025-11-28T16:23:28.039796Z","iopub.status.idle":"2025-11-28T16:23:28.050393Z","shell.execute_reply.started":"2025-11-28T16:23:28.039770Z","shell.execute_reply":"2025-11-28T16:23:28.049632Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## I split the dataset set into train and test sets as usual.","metadata":{"id":"uDwobMu_okwv"}},{"cell_type":"code","source":"# source = input(\"Choose data source [tfds/kaggle]: \").strip().lower()\nConfigs.Source = \"huggingface\"\n\nif Configs.Source not in (\"tfds\", \"kaggle\", \"huggingface\"):\n  print(\"Invalid choice. Defaulting to 'tfds'.\")\n  source = \"\"\n\nprint(f\"Using data source: {Configs.Source}\")\n\ndataset = get_dataset(Configs.TRAIN_DATA_DIR, \"train\", Configs.Source).batch(Configs.TRAIN_MICRO_BATCH_SIZE)[\n    :Configs.NUM_BATCHES\n]\n\nif Configs.TRAIN_FRACTION == 1.0:\n  train_dataset = dataset.repeat(Configs.NUM_EPOCHS)\n  val_dataset = None\nelse:\n  train_dataset = dataset[: int(len(dataset) * Configs.TRAIN_FRACTION)]\n  train_dataset = train_dataset.repeat(Configs.NUM_EPOCHS)\n\n  val_dataset = dataset[int(len(dataset) * Configs.TRAIN_FRACTION) :].repeat(Configs.NUM_EPOCHS)\n\ntest_dataset = get_dataset(Configs.TEST_DATA_DIR, \"test\", Configs.Source).batch(Configs.TRAIN_MICRO_BATCH_SIZE)[\n    :Configs.NUM_TEST_BATCHES\n]\n\ndataset_lengths = (\n    len(train_dataset),\n    len(val_dataset) if val_dataset is not None else 0,\n    len(test_dataset),\n)\nprint(f\"dataset contains {dataset_lengths} of batches\")","metadata":{"id":"KXhOL6GyTn1k","outputId":"5e15f893-33eb-42e9-f4bd-20be01f2314a","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:23:28.858247Z","iopub.execute_input":"2025-11-28T16:23:28.858910Z","iopub.status.idle":"2025-11-28T16:23:33.346951Z","shell.execute_reply.started":"2025-11-28T16:23:28.858886Z","shell.execute_reply":"2025-11-28T16:23:33.346142Z"}},"outputs":[{"name":"stdout","text":"Using data source: huggingface\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aa26e03d13b46d09f09aa7e0fd8626c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0a0fd25a104639917724be58026888"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e1efde97bf142049ec6fad3a4cbe942"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25db420801fe4e99ad14cdfda9982e5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edb49167c4624352a60f4292eef391c5"}},"metadata":{}},{"name":"stdout","text":"dataset contains (2990, 748, 100) of batches\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Let's see how one batch of the training dataset looks like!\n","metadata":{"id":"k7n8L0VzTn1k"}},{"cell_type":"code","source":"for ele in train_dataset[:1]:\n  pprint(ele)","metadata":{"id":"5TF-wNQ2Tn1k","outputId":"367cd3ef-9b1c-469d-b50c-71887b040e87","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:23:33.348210Z","iopub.execute_input":"2025-11-28T16:23:33.348714Z","iopub.status.idle":"2025-11-28T16:23:33.356076Z","shell.execute_reply.started":"2025-11-28T16:23:33.348695Z","shell.execute_reply":"2025-11-28T16:23:33.355208Z"}},"outputs":[{"name":"stdout","text":"{'answer': array(['3', '34', '300', '35'], dtype='<U3'),\n 'prompts': array(['<start_of_turn>user\\n    You are given a problem. Think about the problem and     provide your reasoning. Place it between <reasoning> and     </reasoning>. Then, provide the final answer (i.e., just one numerical     value) between <answer> and </answer>.\\n    \\n    Maria has 4 dimes, 4 quarters, and 7 nickels in her piggy bank. Her mom gives her 5 quarters. How much money, in dollars, does Maria have now?<end_of_turn>\\n    <start_of_turn>model',\n       '<start_of_turn>user\\n    You are given a problem. Think about the problem and     provide your reasoning. Place it between <reasoning> and     </reasoning>. Then, provide the final answer (i.e., just one numerical     value) between <answer> and </answer>.\\n    \\n    A wildlife team is monitoring the number of birds in a park. There are 3 blackbirds in each of the park’s 7 trees. There are also 13 magpies roaming around the park. How many birds are in the park in total?<end_of_turn>\\n    <start_of_turn>model',\n       '<start_of_turn>user\\n    You are given a problem. Think about the problem and     provide your reasoning. Place it between <reasoning> and     </reasoning>. Then, provide the final answer (i.e., just one numerical     value) between <answer> and </answer>.\\n    \\n    Last year, the school library purchased 50 new books. This year, it purchased 3 times as many books. If the library had 100 books before it purchased new books last year, how many books are in the library now?<end_of_turn>\\n    <start_of_turn>model',\n       '<start_of_turn>user\\n    You are given a problem. Think about the problem and     provide your reasoning. Place it between <reasoning> and     </reasoning>. Then, provide the final answer (i.e., just one numerical     value) between <answer> and </answer>.\\n    \\n    Jame gets 20 singing lessons.  He gets the first lesson free and after the first 10 paid lessons he only needs to pay for every other lesson.  Each lesson is $5.  His uncle pays for half.  How much does James pay?<end_of_turn>\\n    <start_of_turn>model'],\n      dtype='<U516'),\n 'question': array(['Maria has 4 dimes, 4 quarters, and 7 nickels in her piggy bank. Her mom gives her 5 quarters. How much money, in dollars, does Maria have now?',\n       'A wildlife team is monitoring the number of birds in a park. There are 3 blackbirds in each of the park’s 7 trees. There are also 13 magpies roaming around the park. How many birds are in the park in total?',\n       'Last year, the school library purchased 50 new books. This year, it purchased 3 times as many books. If the library had 100 books before it purchased new books last year, how many books are in the library now?',\n       'Jame gets 20 singing lessons.  He gets the first lesson free and after the first 10 paid lessons he only needs to pay for every other lesson.  Each lesson is $5.  His uncle pays for half.  How much does James pay?'],\n      dtype='<U213')}\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Load the policy model and the reference model\n\nThe policy model is the model which is actually trained and whose weights are\nupdated. The reference model is the model with which we compute KL divergence.\n\nTypically, the reference model is the base model, and the policy model is the\nsame base model, but with the LoRA parameters added to it. Only the LoRA parameters are updated. So (if you are familiar with the deepseek notation [DeepSeek](https://arxiv.org/pdf/2501.12948) ) Pi_old and Pi_theta will have the full model + the LoRa weights and the Pi_ref is just the original model. \n\nNote: We perform full precision (fp32) training. You can, however, leverage\nQwix for QAT.\n\nTo load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\nto have agreed to the Gemma license\n[here](https://www.kaggle.com/models/google/gemma/flax/).","metadata":{"id":"BZxBR7Y_Tn1k"}},{"cell_type":"code","source":"os.environ[\"KAGGLE_KEY\"] = UserSecretsClient().get_secret(\"KAGGLE_KEY\")\nos.environ[\"KAGGLE_USERNAME\"] = UserSecretsClient().get_secret(\"KAGGLE_USERNAME\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:30:41.064116Z","iopub.execute_input":"2025-11-28T16:30:41.064429Z","iopub.status.idle":"2025-11-28T16:30:41.360266Z","shell.execute_reply.started":"2025-11-28T16:30:41.064408Z","shell.execute_reply":"2025-11-28T16:30:41.359493Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"# Log in\nif \"KAGGLE_USERNAME\" not in os.environ.keys() or \"KAGGLE_KEY\" not in os.environ.keys():\n  kagglehub.login()","metadata":{"id":"3GfLHHVYHHKO","outputId":"3290c9e8-2362-44a1-93bf-9a5caa7d201f","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:30:43.174979Z","iopub.execute_input":"2025-11-28T16:30:43.175672Z","iopub.status.idle":"2025-11-28T16:30:43.179074Z","shell.execute_reply.started":"2025-11-28T16:30:43.175648Z","shell.execute_reply":"2025-11-28T16:30:43.178342Z"}},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"This code snippet serves as a workaround to re-save the pre-trained model checkpoint from Kaggle into a local format that is compatible with the [Flax NNX](https://flax.readthedocs.io/en/stable/why.html) library. Because the original checkpoint has parameter names and tensor structures that don't match the target NNX model architecture, it cannot be loaded directly.\n\nSo let's first load the original weights into a temporary model instance, then extract and re-save the model's state into a new, properly formatted local checkpoint, which can then be successfully loaded by the final sharded NNX model.","metadata":{"id":"nAghcsT_Pmv_"}},{"cell_type":"code","source":"!rm /tmp/content/intermediate_ckpt/* -rf\n\n!rm /tmp/content/ckpts/* -rf\n\nmodel_family = \"gemma3\"\nif model_family == \"gemma3\":\n  Configs.MODEL_CP_PATH = params.GEMMA3_1B_IT\n  config = model.ModelConfig.gemma3_1b()\n  gemma = params.create_model_from_checkpoint(Configs.MODEL_CP_PATH, config)\n  tokenizer = params.create_tokenizer()\n\n  checkpointer = ocp.StandardCheckpointer()\n  ## nnx.split splits the single module instance into two separate objects:\n  ##1 - Variables/state (params, batch stats, rngs, etc.)\n  ##2 - The pure callable module (the “function” part)\n  _, state = nnx.split(gemma)\n  checkpointer.save(os.path.join(Configs.INTERMEDIATE_CKPT_DIR, \"state\"), state)\n  checkpointer.wait_until_finished()\n  # Delete the intermediate model to save memory.\n  del params\n  del gemma\n  del state\n  gc.collect()","metadata":{"id":"cIFAxgVOTn1k","outputId":"0235a0e1-9f7d-428c-c16e-0bc9e49c2f2f","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T16:33:26.755165Z","iopub.execute_input":"2025-11-28T16:33:26.755694Z","iopub.status.idle":"2025-11-28T16:33:27.077319Z","shell.execute_reply.started":"2025-11-28T16:33:26.755664Z","shell.execute_reply":"2025-11-28T16:33:27.076151Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_38/707972266.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mConfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL_CP_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGEMMA3_1B_IT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemma3_1b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mgemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_model_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL_CP_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tunix/models/gemma3/params.py\u001b[0m in \u001b[0;36mcreate_model_from_checkpoint\u001b[0;34m(checkpoint_path, model_config, mesh, dtype)\u001b[0m\n\u001b[1;32m     57\u001b[0m ) -> model_lib.Gemma3:\n\u001b[1;32m     58\u001b[0m   \u001b[0;34m\"\"\"Load a Gemma3 model from a checkpoint.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   abs_model = nnx.eval_shape(\n\u001b[0m\u001b[1;32m     60\u001b[0m       \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGemma3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRngs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m   )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/transforms/transforms.py\u001b[0m in \u001b[0;36meval_shape\u001b[0;34m(f, *args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m   \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_eval_shape_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","    \u001b[0;31m[... skipping hidden 14 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/transforms/transforms.py\u001b[0m in \u001b[0;36m_eval_shape_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_eval_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_duplicates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tunix/models/gemma3/params.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0;34m\"\"\"Load a Gemma3 model from a checkpoint.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m   abs_model = nnx.eval_shape(\n\u001b[0;32m---> 60\u001b[0;31m       \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGemma3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrngs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRngs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m   )\n\u001b[1;32m     62\u001b[0m   \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mocp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStandardCheckpointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/pytreelib.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_graph_node_meta_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_pytree_meta_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/pytreelib.py\u001b[0m in \u001b[0;36m_graph_node_meta_call\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0mvars_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_pytree__state'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPytreeState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m   \u001b[0mvars_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_pytree__nodes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree__nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree_meta_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree__is_pytree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/pytreelib.py\u001b[0m in \u001b[0;36m_pytree_meta_construct\u001b[0;34m(cls, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_pytree_meta_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0mObjectMeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPytreeMeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/rnglib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, default, **rngs)\u001b[0m\n\u001b[1;32m    367\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRngStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m       stream = RngStream(\n\u001b[0m\u001b[1;32m    370\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/pytreelib.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_graph_node_meta_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_pytree_meta_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/pytreelib.py\u001b[0m in \u001b[0;36m_graph_node_meta_call\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0mvars_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_pytree__state'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPytreeState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m   \u001b[0mvars_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_pytree__nodes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree__nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m   \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree_meta_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pytree__is_pytree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \u001b[0mmissing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/pytreelib.py\u001b[0m in \u001b[0;36m_pytree_meta_construct\u001b[0;34m(cls, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_pytree_meta_construct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0mObjectMeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPytreeMeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/flax/nnx/rnglib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, key, tag)\u001b[0m\n\u001b[1;32m     57\u001b[0m   ):\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m       \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_key_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/random.py\u001b[0m in \u001b[0;36mkey\u001b[0;34m(seed, impl)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mfold_in\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m   \"\"\"\n\u001b[0;32m--> 220\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'key'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m def PRNGKey(seed: int | ArrayLike, *,\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/random.py\u001b[0m in \u001b[0;36m_key\u001b[0;34m(ctor_name, seed, impl_spec)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;34mf\"{ctor_name} accepts a scalar seed, but was given an array of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         f\"shape {np.shape(seed)} != (). Use jax.vmap for batching\")\n\u001b[0;32m--> 200\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mprng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m def key(seed: int | ArrayLike, *,\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/prng.py\u001b[0m in \u001b[0;36mrandom_seed\u001b[0;34m(seeds, impl)\u001b[0m\n\u001b[1;32m    552\u001b[0m   \u001b[0;31m# use-case of instantiating with Python hashes in X32 mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m     \u001b[0mseeds_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0mseeds_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/array_constructors.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, copy, device, out_sharding)\u001b[0m\n\u001b[1;32m    399\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_and_canonicalize_user_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"asarray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m   return array(a, dtype=dtype, copy=bool(copy), order=order, device=device,\n\u001b[0m\u001b[1;32m    402\u001b[0m                out_sharding=out_sharding)\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/numpy/array_constructors.py\u001b[0m in \u001b[0;36marray\u001b[0;34m(object, dtype, copy, order, ndmin, device, out_sharding)\u001b[0m\n\u001b[1;32m    284\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected input type for array: {type(object)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m   out_array: Array = lax._convert_element_type(\n\u001b[0m\u001b[1;32m    287\u001b[0m       out, dtype, weak_type=weak_type, sharding=sharding)\n\u001b[1;32m    288\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mndmin\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/jax/_src/literals.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;31m# TODO(phawkins): revert to the above form once NumPy 2.3 is the minimum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;31m# supported version.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=wrong-keyword-args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__add__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: asarray() got an unexpected keyword argument 'copy'"],"ename":"TypeError","evalue":"asarray() got an unexpected keyword argument 'copy'","output_type":"error"}],"execution_count":33},{"cell_type":"markdown","source":"### Model Loading and LoRA Application\n\nThese two functions work together to load a base model from a checkpoint and apply a LoRA (Low-Rank Adaptation) layer to it.\n\n* `get_ref_model`: Loads the complete Gemma model from a specified checkpoint path. It uses **JAX sharding** to distribute the model parameters across multiple devices (usually TPU cores).\n* \n* `get_lora_model`: Takes the base model and applies LoRA layers to it. It uses a `LoraProvider` to select specific layers/transformer-sub-modules (like attention and MLP layers) to be adapted. The resulting LoRA-infused model is then sharded and updated to ensure it's ready for distributed training.","metadata":{"id":"hpgXONuORkkq"}},{"cell_type":"code","source":"from tunix.models.gemma3 import params\n\ndef get_gemma_ref_model(ckpt_path):\n  mesh = jax.make_mesh(*Configs.MESH)\n  model_config = model.ModelConfig.gemma3_1b()\n  abs_gemma: nnx.Module = nnx.eval_shape(\n      lambda: params.create_model_from_checkpoint(Configs.MODEL_CP_PATH, config)\n  )\n\n  abs_state = nnx.state(abs_gemma)\n  abs_state = jax.tree.map(\n      lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n      abs_state,\n      nnx.get_named_sharding(abs_state, mesh),\n  )\n  checkpointer = ocp.StandardCheckpointer()\n  restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n\n  graph_def, _ = nnx.split(abs_gemma)\n  gemma = nnx.merge(graph_def, restored_params)\n  return gemma, mesh, model_config\n\n\ndef get_lora_model(base_model, mesh):\n  lora_provider = qwix.LoraProvider(\n      module_path=(\n          \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n          \".*attn_vec_einsum\"\n      ),\n      rank=Configs.RANK,\n      alpha=Configs.ALPHA,\n  )\n\n  model_input = base_model.get_model_input()\n  lora_model = qwix.apply_lora_to_model(\n      base_model, lora_provider, **model_input\n  )\n\n  with mesh:\n    state = nnx.state(lora_model)\n    pspecs = nnx.get_partition_spec(state)\n    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n    nnx.update(lora_model, sharded_state)\n\n  return lora_model","metadata":{"id":"m2KD-nmbTn1k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import flax\nflax.__version__","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load the reference and policy Gemma models using the Flax NNX library and display their structures. \n\n### Remember that the reference Model will stay untouched, it is only used to compute the KL Divergence at each time step, while the policy model uses LoRa, meaning it is the model whose weights will change.","metadata":{"id":"mgBALRieR6aY"}},{"cell_type":"code","source":"# Reference model\nif model_family == \"gemma3\":\n  ref_model, mesh, model_config = get_gemma_ref_model(\n      ckpt_path=os.path.join(Configs.INTERMEDIATE_CKPT_DIR, \"state\")\n  )","metadata":{"id":"kSdZ7aGhTn1k","outputId":"a536819f-dd5f-4e29-8ebe-09234960c114","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Policy model\nlora_policy = get_lora_model(ref_model, mesh=mesh)\n# nnx.display(lora_policy)","metadata":{"id":"4i3CfJ1gTn1k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Define reward functions\n\nThe original Tunix team defined four reward functions:\n\n- reward if the format of the output exactly matches the instruction given in\n`TEMPLATE`;\n- reward if the format of the output approximately matches the instruction given\nin `TEMPLATE`;\n- reward if the answer is correct/partially correct;\n- Sometimes, the text between `<answer>`, `</answer>` might not be one\n  number. So, we extract the number, and reward the model if the answer is correct.\n\nThe reward functions are inspired from\n[here](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb), but they also seem to be somewhat in line with the [deepseek](https://arxiv.org/pdf/2501.12948) paper (section 2.2.2).\n\nFará sentido beneficiar assim tanto o formato ? talvez, mas tenho de verificar se o formato é avaliado a dobrar, pq podem usar a primeira e a segunda reward funtion, o que significa que o total de recompensa pelo formato completo é de 7 valores (parece me alto). De resto é assim que devemos ensinar o modelo a pensar a um nível mais profundo? verificar que nos deixou tokens a dizer que houve reasoning ou não ? será que é assim que vamos aproximar o raciocínio da máquina ao nosso ?\n\n## A parte das reward funtions é a mais importante do treino, é o que vai definir o objectivo do treino, por isso devemos recompensar raciocínio que se assemlhe ao nosso e não apenas formato e respostas certas. \n\n### Como raciocinamos nós ? \nProblema : Tens três interruptores fora de uma sala fechada — A, B e C — e dentro da sala há três lâmpadas (cada interruptor controla exactamente uma lâmpada). Podes mexer nos interruptores o quanto quiseres, mas só podes entrar na sala uma vez (depois de entrares não podes voltar a mexer nos interruptores). Como descobres qual interruptor controla qual lâmpada?\n\nLinha de pensamento : \n##### 1 - recolher e organizar a informação chave \n(Cenário : 3 interruptores ligados cada um a uma lâmpada apenas. Lâmpadas dentro de uma sala fechada. Objectivo : Descobrir que interruptor se liga a qual lâmpada. Constragimento : só posso entrar na sala uma vez e não posso mexer nos interruptores depois de o fazer.\n##### 2 - Analisar caminhos de ação possíveis com base nessa info e no meu conhecimento\n( Cenário de ação 1 : ligar todos os interruptores, leva a uma inconlusão porque continua a ser impossível associar um interruptor a uma lâmpada, Cenário 2 : ligar apenas uma lâmpada; tb será inconclusivo pois só me dá info sobre uma das associações . Cenário 3 : ligar duas lêmpadas, idem... etc)\n##### 3 - Com base na análise providenciar uma resposta final. \nNeste caso, não sei como o fazer, isto é uma conclusão válida que me vai reencaminhar de novo para o ponto 2, procurar novos cenário de ação. Se eu fosse uma máquina continuaria a iterar até acabar.\n\n### Como atribuir recompensas quando os critérios são mais vagos ?\n\nSeria muito mais interessante avaliar as respostas e atribuir uma recompensa com base nestes critérios, ou seja, ver se o LLM recolheu e organizou info chave, analisou caminhos de ação possíveis com base na info chave e no seu conhecimento e por fim se tentou chegar a uma resposta (iterando quando não a achou). \n\nAqui, se calhar, até deixaria a resposta certa com um peso muito mais baixo, pouco me interessa que ele acerte as perguntas durante o treino, possivelmente já as viu, quero é dar ênfase ao processo de reflexão. Embora a resposta certa deva estar presente na Reward Funtion, porque assume-se que um bom processo de pensamento leve mais vezes a uma resposta certa.\n\nSe calhar a forma mais eficiente de atribuir uma recompensa neste caso é mesmo usando um LLM as a judge, porque é impossível avaliar isto se outra forma; a não ser que construamos um classificador que o faça.\n\n","metadata":{"id":"zLzR1tJfTn1k"}},{"cell_type":"code","source":"from google import genai\nfrom pydantic import BaseModel, Field\n\nclass Rewards(BaseModel):\n    reward_info_gathering_and_organization: int = Field(\n        description=\"The reward associated with the ability to gather info and organize it\"\n    )\n    reward_analyse_possible_action_paths: int = Field(\n        description=\"The reward associated with analyzing possible action paths\"\n    )\n    reward_choose_an_action_path_and_return_an_answer_related_to_it: int = Field(\n        description=\"The reward associated with choosing an action path and returning an answer\"\n    )\n\nclass GeminiJudge:\n    def __init__(self, api_key: str):\n        self.client = genai.Client(api_key=api_key)\n\n    def llm_as_judge(self, input_prompt: str) -> Rewards:\n        response = self.client.models.generate_content(\n            model=\"gemini-2.5-flash\",\n            contents=input_prompt,\n            config={\n                \"response_mime_type\": \"application/json\",\n                \"response_json_schema\": Rewards.model_json_schema(),\n            },\n        )\n        rewards = Rewards.model_validate_json(response.text)\n        return rewards\n\n\n# Example usage\njudge = GeminiJudge(api_key=\"YOUR_API_KEY\")\nprompt = \"Judge this action sequence and assign rewards for gathering, analyzing, and choosing.\"\nreward_values = judge.llm_as_judge(prompt)\nprint(reward_values)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# First off, let's define a RegEx for checking whether the format matches.\nmatch_format = re.compile(\n    rf\"^[\\s]{{0,}}\"\n    rf\"{Configs.reasoning_start}.+?{Configs.reasoning_end}.*?\"\n    rf\"{Configs.solution_start}(.+?){Configs.solution_end}\"\n    rf\"[\\s]{{0,}}$\",\n    flags=re.MULTILINE | re.DOTALL,\n)\n\nmatch_format.search(\n    f\"{Configs.reasoning_start}Let me\"\n    f\" think!{Configs.reasoning_end}{Configs.solution_start}2{Configs.solution_end}\",\n)","metadata":{"id":"C7Beft8wTn1k","outputId":"a0ba4233-562d-485d-b9ba-2f22cf2785b4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Give the model a reward of 3 points if the format matches exactly.","metadata":{"id":"Fe1rF15zTn1k"}},{"cell_type":"code","source":"def match_format_exactly(prompts, completions, **kwargs):\n  return [\n      0 if match_format.search(response) is None else 3.0\n      for response in completions\n  ]","metadata":{"id":"_fhQ6pY2Tn1k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Reward the model if the format of the output matches partially.","metadata":{"id":"sWdAdUHuTn1k"}},{"cell_type":"code","source":"def match_format_approximately(prompts, completions, **kwargs):\n  scores = []\n\n  for completion in completions:\n    score = 0\n    response = completion\n    # Count how many keywords are seen - we penalize if too many!\n    # If we see 1, then plus some points!\n    score += 0.5 if response.count(Configs.reasoning_start) == 1 else -0.5\n    score += 0.5 if response.count(Configs.reasoning_end) == 1 else -0.5\n    score += 0.5 if response.count(Configs.solution_start) == 1 else -0.5\n    score += 0.5 if response.count(Configs.solution_end) == 1 else -0.5\n    scores.append(score)\n  return scores","metadata":{"id":"uOhO4f3-Tn1k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Reward the model if the answer is correct. A reward is also given if the answer does not match exactly, i.e., based on how close the answer is to the correct value.","metadata":{"id":"A2fNZDgTTn1k"}},{"cell_type":"code","source":"def check_answer(prompts, completions, answer, **kwargs):\n  responses = completions\n\n  extracted_responses = [\n      guess.group(1) if (guess := match_format.search(r)) is not None else None\n      for r in responses\n  ]\n\n  scores = []\n  assert len(extracted_responses) == len(\n      answer\n  ), f\"{extracted_responses} and {answer} have mismatching length\"\n  for guess, true_answer in zip(extracted_responses, answer):\n    score = 0\n    if guess is None:\n      scores.append(0)\n      continue\n    # Correct answer gets 3 points!\n    if guess == true_answer:\n      score += 3.0\n    # Match if spaces are seen\n    elif guess.strip() == true_answer.strip():\n      score += 1.5\n    else:\n      # We also reward it if the answer is close via ratios!\n      # Ie if the answer is within some range, reward it!\n      try:\n        ratio = float(guess) / float(true_answer)\n        if ratio >= 0.9 and ratio <= 1.1:\n          score += 0.5\n        elif ratio >= 0.8 and ratio <= 1.2:\n          score += 0.25\n        else:\n          score -= 1.0  # Penalize wrong answers\n      except:\n        score -= 0.5  # Penalize\n    scores.append(score)\n  return scores","metadata":{"id":"S8zcWsmhTn1k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Sometimes, the text between `<answer>` and `</answer>` might not be one\nnumber; it can be a sentence. So, we extract the number and compare the answer.","metadata":{"id":"nIpOVv78Tn1k"}},{"cell_type":"code","source":"match_numbers = re.compile(\n    rf\"{solution_start}.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL\n)\nmatch_numbers.findall(f\"{solution_start}  0.34  {solution_end}\")","metadata":{"id":"NXvRtbk8Tn1k","outputId":"1ab45f0e-d04a-455c-a046-7c7a1ec6ee22","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_numbers(prompts, completions, answer, **kwargs):\n  question = kwargs[\"question\"]\n  responses = completions\n\n  extracted_responses = [\n      guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n      for r in responses\n  ]\n\n  scores = []\n  print(\"START ============================\")\n  print(f\"Question: {question[0]}\")\n  print(f\"Answer: {answer[0]}\")\n  print(f\"Response: {responses[0]}\")\n  print(f\"Extracted: {extracted_responses[0]}\")\n  print(\"END ==============================\")\n  for guess, true_answer in zip(extracted_responses, answer):\n    if guess is None:\n      scores.append(0)\n      continue\n    # Convert to numbers\n    try:\n      true_answer = float(true_answer.strip())\n      guess = float(guess.strip())\n      scores.append(1.5 if guess == true_answer else 0.0)\n    except:\n      scores.append(0)\n      continue\n  return scores","metadata":{"id":"oxZQAFKOTn1k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate\n\n\nBefore we train the model, let's evaluate the model on the test set so we can\nsee the improvement post training.\n\nWe evaluate it in two ways:\n\n**Quantitative**\n\n* **Answer Accuracy**: percentage of samples for which the model predicts the\ncorrect final numerical answer  \n* **Answer (Partial) Accuracy**: percentage of samples for which the model\npredicts a final numerical answer such that the \\`model answer / answer\\`\nratio lies between 0.9 and 1.1.  \n* **Format Accuracy**: percentage of samples for which the model outputs the\ncorrect format, i.e., reasoning between the reasoning special tokens, and the\nfinal answer between the \\`\\<start\\_answer\\>\\`, \\`\\<end\\_answer\\>\\` tokens.\n\n**Qualitative**\n\nWe'll also print outputs for a few given questions so that we can compare the generated output later.\n","metadata":{"id":"AaiYMJxFTn1k"}},{"cell_type":"markdown","source":"We define a helper function to generate an answer, given a prompt.","metadata":{"id":"HAaZ7NjBx99P"}},{"cell_type":"code","source":"def generate(\n    question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None\n):\n  \"\"\"Given prompt, generates text.\"\"\"\n\n  if isinstance(question, str):\n    input_batch = [\n        TEMPLATE.format(\n            system_prompt=SYSTEM_PROMPT,\n            question=question,\n        ),\n    ]\n  else:\n    input_batch = [\n        TEMPLATE.format(\n            system_prompt=SYSTEM_PROMPT,\n            question=q,\n        )\n        for q in question\n    ]\n\n  out_data = sampler(\n      input_strings=input_batch,\n      max_generation_steps=768,\n      temperature=temperature,\n      top_k=top_k,\n      top_p=top_p,\n      echo=False,\n      seed=seed if seed is not None else None,\n      eos_tokens=[1,106],\n  )\n\n  output = out_data.text\n  if isinstance(question, str):\n    return output[0]\n  return output","metadata":{"id":"_k58bOicUHJy","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Another helper function for evaluation.","metadata":{"id":"zNoa5je7yJOt"}},{"cell_type":"code","source":"def evaluate(\n    dataset,\n    sampler,\n    temperature=0.7,\n    top_k=50,\n    top_p=0.95,\n    num_passes=1,\n    corr_lst=False,\n    make_lst=False,\n):\n  \"\"\"Computes accuracy and percentage of outputs matching the format.\"\"\"\n\n  response_lst = []\n  corr = 0\n  partially_corr = 0\n  corr_format = 0\n  total = 0\n\n  for batch in tqdm(dataset):\n    answers = batch[\"answer\"]\n    questions = batch[\"question\"]\n\n    multiple_call_responses = [[] for _ in range(len(questions))]\n    for p in range(num_passes):\n      responses = generate(\n          questions, sampler, temperature, top_k, top_p, seed=p\n      )\n      for idx, response in enumerate(responses):\n        multiple_call_responses[idx].append(response)\n\n    for question, multiple_call_response, answer in zip(\n        questions, multiple_call_responses, answers\n    ):\n      # check answer\n      corr_ctr_per_question = 0\n      partially_corr_per_question = 0\n      corr_format_per_question = 0\n      for response in multiple_call_response:\n        extracted_response = (\n            guess.group(1)\n            if (guess := match_numbers.search(response)) is not None\n            else \"-1000000\"\n        )\n        try:\n          if float(extracted_response.strip()) == float(answer.strip()):\n            corr_ctr_per_question += 1\n\n          ratio = float(extracted_response.strip()) / float(answer.strip())\n          if ratio >= 0.9 and ratio <= 1.1:\n            partially_corr_per_question += 1\n        except:\n          print(\"SKIPPED\")\n\n        # check format\n        if match_format.search(response) is not None:\n          corr_format_per_question += 1\n\n        if (\n            corr_ctr_per_question > 0\n            and partially_corr_per_question > 0\n            and corr_format_per_question > 0\n        ):\n          break\n\n      if corr_ctr_per_question > 0:\n        corr += 1\n        if corr_lst and make_lst:\n          response_lst.append((question, answer, multiple_call_response))\n      else:\n        if not corr_lst and make_lst:\n          response_lst.append((question, answer, multiple_call_response))\n      if partially_corr_per_question > 0:\n        partially_corr += 1\n      if corr_format_per_question > 0:\n        corr_format += 1\n\n      total += 1\n      if total % 10 == 0:\n        print(\n            f\"===> {corr=}, {total=}, {corr / total * 100=}, \"\n            f\"{partially_corr / total * 100=}, {corr_format / total * 100=}\"\n        )\n\n  to_return = (\n      corr,\n      total,\n      corr / total * 100,\n      partially_corr / total * 100,\n      corr_format / total * 100,\n  )\n  if make_lst:\n    return to_return, response_lst\n  return to_return","metadata":{"id":"yJo2nuKB-wlw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)","metadata":{"id":"HZMO-KflTn1k","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now let's see how the original model does on the test set. You can see the percentages of the mode outputs that are fully correct, partially correct and just correct in format. The following step might take couple of minutes to finish.","metadata":{"id":"UOAQe06DyVlQ"}},{"cell_type":"code","source":"# The evaluation might take up to couple of minutes to finish. Please be patient.\n\n(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n    test_dataset,\n    sampler,\n    **GENERATION_CONFIGS[\"greedy\"],\n)\nprint(\n    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n    f\" {format_accuracy=}%\"\n)","metadata":{"id":"YQM-tzXWUmoE","outputId":"d125e6b1-f940-44b3-a05d-f837d2299f49","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train\n\nLet's set up all the configs first - checkpointing, metric logging and training.\nWe then train the model.","metadata":{"id":"-CmB2ZT9Tn1l"}},{"cell_type":"code","source":"# Ckpt saving\ncheckpointing_options = ocp.CheckpointManagerOptions(\n    save_interval_steps=SAVE_INTERVAL_STEPS, max_to_keep=MAX_TO_KEEP\n)\n\n# Metrics logger\nmetrics_logging_options = metrics_logger.MetricsLoggerOptions(\n    log_dir=\"/tmp/content/tmp/tensorboard/grpo\", flush_every_n_steps=20\n)","metadata":{"id":"mHzdsYsGTn1l","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optimizer, learning rate scheduler, gradient clipping\noptimizer = optax.adamw(\n    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n        init_value=0.0,\n        peak_value=LEARNING_RATE,\n        warmup_steps=WARMUP_STEPS,\n        decay_steps=MAX_STEPS,\n        end_value=0.0,\n    ),\n    b1=B1,\n    b2=B2,\n    weight_decay=WEIGHT_DECAY,\n)\nif MAX_GRAD_NORM is not None:\n  optimizer = optax.chain(\n      optax.clip_by_global_norm(max_norm=MAX_GRAD_NORM),\n      optimizer,\n  )","metadata":{"id":"YWvBkWBsruom","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training config\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={\n        rl_cluster_lib.Role.ACTOR: mesh,\n        rl_cluster_lib.Role.REFERENCE: mesh,\n        rl_cluster_lib.Role.ROLLOUT: mesh,\n    },\n    rollout_engine='vanilla',\n    offload_to_cpu=False,\n    training_config=rl_cluster_lib.RLTrainingConfig(\n        actor_optimizer=optimizer,\n        eval_every_n_steps=EVAL_EVERY_N_STEPS,\n        max_steps=MAX_STEPS,\n        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        # metrics logging\n        metrics_logging_options=metrics_logging_options,\n        # checkpoint saving\n        checkpoint_root_directory=CKPT_DIR,\n        checkpointing_options=checkpointing_options,\n    ),\n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n        max_prompt_length=MAX_PROMPT_LENGTH,\n        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        temperature=TEMPERATURE,\n        top_p=TOP_P,\n        top_k=TOP_K,\n        eos_tokens=[1,106],\n    ),\n)\n\ngrpo_config = GRPOConfig(\n    num_generations=NUM_GENERATIONS,\n    num_iterations=NUM_ITERATIONS,\n    beta=BETA,\n    epsilon=EPSILON,\n)","metadata":{"id":"_6VxFW1ZTn1l","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Setting Up the GRPO Trainer\n\nNow we initialize our system for training. First, we create an `RLCluster` instance, which brings together the **policy model (`actor`)**, a **reference model (`reference`)**, and a **tokenizer**. Our `actor` is a trainable LoRA model, while the `reference` is a fixed base model that we use to guide the training.\n\nWe then create a `GRPOLearner`, the specialized trainer that uses a list of **reward functions** to evaluate and optimize the model's output, completing the RL training setup.\n\nTunix trainers are integrated with [Weights & Biases](https://wandb.ai/) to help you visualize the training progress. You can choose how you want to use it:\n\n**Option 1 (Type 1)**: If you're running a quick experiment or just testing things out, choose this. It creates a temporary, private dashboard right in your browser without requiring you to log in or create an account.\n\n**Option 2 (Type 2)**: If you have an existing W&B account and want to save your project's history to your personal dashboard, choose this. You'll be prompted to enter your API key or log in.","metadata":{"id":"z4yJWiElSmOy"}},{"cell_type":"code","source":"# RL cluster\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor=lora_policy,\n    reference=ref_model,\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n)\n\n# GRPO Trainer\ngrpo_trainer = GRPOLearner(\n    rl_cluster=rl_cluster,\n    reward_fns=[\n        match_format_exactly,\n        match_format_approximately,\n        check_answer,\n        check_numbers,\n    ],\n    grpo_config=grpo_config,\n)","metadata":{"id":"OIe1lO08Tn1l","outputId":"017a2a3f-d9fd-4ac8-87f5-760e2586dfc9","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The first couple of training step might take up to 5 minutes to finish. Please be patient. If you experience long training steps, e.g. >10 minutes per step, please open a bug. Really appreciated!","metadata":{"id":"e8b71ed5"}},{"cell_type":"code","source":"with mesh:\n  grpo_trainer.train(train_dataset)","metadata":{"id":"S27XDebYTn1l","outputId":"2869f47f-c7d7-4bcc-b167-f05c77a619fa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluate\n\nLet's evaluate our finetuned model!","metadata":{"id":"FzIP8glkTn1l"}},{"cell_type":"code","source":"# Load checkpoint first.\nimport re\n\n# Find the latest checkpoint by listing directories in CKPT_DIR/actor\nactor_ckpt_dir = os.path.join(CKPT_DIR, \"actor\")\n\nlatest_step = -1\nif os.path.exists(actor_ckpt_dir):\n  for item in os.listdir(actor_ckpt_dir):\n    if os.path.isdir(os.path.join(actor_ckpt_dir, item)) and re.match(r'^\\d+$', item):\n      step = int(item)\n      if step > latest_step:\n        latest_step = step\n\nif latest_step == -1:\n  raise FileNotFoundError(f\"No checkpoints found in {actor_ckpt_dir}\")\n\nprint(f\"Latest checkpoint step: {latest_step}\")\n\nwandb.init(project='tunix-eval')  # logging bug workaround\n\ntrained_ckpt_path = os.path.join(\n    CKPT_DIR, \"actor\", str(latest_step), \"model_params\"\n)\n\nabs_params = jax.tree.map(\n    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n    nnx.state(lora_policy, nnx.LoRAParam),\n)\ncheckpointer = ocp.StandardCheckpointer()\ntrained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n\nnnx.update(\n    lora_policy,\n    jax.tree.map(\n        lambda a, b: b,\n        nnx.state(lora_policy, nnx.LoRAParam),\n        trained_lora_params,\n    ),\n)","metadata":{"id":"V-73HfP1Tn1l","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)","metadata":{"id":"1vY9kl-ITn1l","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The evaluation might take up to couple of minutes to finish. Please be patient.\n(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n    test_dataset,\n    sampler,\n    **GENERATION_CONFIGS[\"greedy\"],\n)\nprint(\n    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n    f\" {format_accuracy=}%\"\n)","metadata":{"id":"nz0q_gGHqYz6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"With sufficient training, you should see that the percentages of correct model outputs have clearly gone up, which means our training worked.","metadata":{"id":"s1NMAxMh0H5D"}},{"cell_type":"markdown","source":"","metadata":{"id":"GKd9Tp1d0dZS"}}]}