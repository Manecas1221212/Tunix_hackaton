{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abdhOBYHqYz6"
   },
   "source": [
    "# GRPO Demo\n",
    "\n",
    "This is a way of training the [Gemma](https://deepmind.google/models/gemma/)\n",
    "3 1B-IT model on the [GSM8K math reasoning benchmark](https://huggingface.co/datasets/openai/gsm8k)\n",
    "using [Group Relative Policy Optimization (GRPO)](https://arxiv.org/pdf/2402.03300).\n",
    "GRPO can enhance your model's problem-solving skills on mathematical word problems,\n",
    "coding problems, etc.\n",
    "\n",
    "This notebook uses a `v5e-8` TPU for Gemma3-1b-it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:20:28.892353Z",
     "iopub.status.busy": "2025-12-02T17:20:28.892027Z",
     "iopub.status.idle": "2025-12-02T17:20:57.810313Z",
     "shell.execute_reply": "2025-12-02T17:20:57.809159Z",
     "shell.execute_reply.started": "2025-12-02T17:20:28.892333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Check for accelerators \n",
    "\n",
    "def is_tpu():\n",
    "    try:\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        devices = xm.get_xla_supported_devices()\n",
    "        return len(devices) > 0\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def is_gpu():\n",
    "    try:\n",
    "        import torch\n",
    "        return torch.cuda.is_available() and not is_tpu()\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "def is_cpu():\n",
    "    return not is_gpu() and not is_tpu()\n",
    "\n",
    "def installs_and_imports():\n",
    "    import os\n",
    "    os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "    if is_tpu() is not False:\n",
    "        print(\"Running on TPU\")\n",
    "        !pip install -q kagglehub\n",
    "        !pip install google-genai\n",
    "        !pip install -q ipywidgets\n",
    "        \n",
    "        !pip install -q tensorflow\n",
    "        !pip install -q tensorflow_datasets\n",
    "        !pip install -q tensorboardX\n",
    "        !pip install -q transformers\n",
    "        !pip install -q grain\n",
    "        !pip install \"google-tunix[prod]==0.1.3\"\n",
    "        \n",
    "        # !pip install -q git+https://github.com/google/tunix\n",
    "        # !pip install -q git+https://github.com/google/qwix\n",
    "        \n",
    "        !pip uninstall -q -y flax\n",
    "        # !pip install -U flax\n",
    "        !pip install flax==0.12.0\n",
    "        !pip install -q datasets wandb==0.22.0\n",
    "\n",
    "        from tunix.generate import sampler as sampler_lib\n",
    "        from tunix.generate import tokenizer_adapter as tokenizer_lib\n",
    "    elif is_gpu() is not False:\n",
    "        print(\"Running on GPU\")\n",
    "        !pip install -q kagglehub ipywidgets tensorflow tensorflow_datasets tensorboardX transformers\n",
    "        !pip install -q grain\n",
    "        !pip install \"google-tunix[prod]==0.1.3\"\n",
    "        !pip uninstall -q -y flax\n",
    "        !pip install flax==0.12.0\n",
    "        !pip install -q datasets wandb==0.22.0\n",
    "        # GPU JAX wheels (replace XXX with your CUDA version)\n",
    "        !pip install --upgrade jax jaxlib==0.4.26+cudaXXX -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "    else:\n",
    "        print(\"Running on CPU, nothing will work :(\")\n",
    "        !pip install -q kagglehub ipywidgets tensorflow tensorflow_datasets tensorboardX transformers\n",
    "        !pip install -q grain\n",
    "        !pip install \"google-tunix[prod]==0.1.3\"\n",
    "        !pip uninstall -q -y flax\n",
    "        !pip install flax==0.12.0\n",
    "        !pip install -q datasets wandb==0.22.0\n",
    "        !pip install jax jaxlib\n",
    "\n",
    "\n",
    "installs_and_imports()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afofSj37qYz6"
   },
   "source": [
    "## Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:20:57.811359Z",
     "iopub.status.busy": "2025-12-02T17:20:57.810893Z",
     "iopub.status.idle": "2025-12-02T17:20:57.900258Z",
     "shell.execute_reply": "2025-12-02T17:20:57.899388Z",
     "shell.execute_reply.started": "2025-12-02T17:20:57.811338Z"
    },
    "id": "953CnpN5xQc_",
    "outputId": "28f62af4-2d67-4034-dc33-d0f69ddf88cc",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb, os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "os.environ['WANDB_API_KEY'] = UserSecretsClient().get_secret(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnF9ZACiTn1k"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:25:08.086707Z",
     "iopub.status.busy": "2025-12-02T17:25:08.086348Z",
     "iopub.status.idle": "2025-12-02T17:25:08.091975Z",
     "shell.execute_reply": "2025-12-02T17:25:08.090820Z",
     "shell.execute_reply.started": "2025-12-02T17:25:08.086687Z"
    },
    "id": "McTNo_r8Tn1k",
    "outputId": "67f038b8-509b-46ed-b027-9b72ed7b628c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import gc\n",
    "import os\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "#When using the version flax==0.12.0, this import works only if TPU is enabled\n",
    "#https://flax.readthedocs.io/en/latest/nnx_basics.html\n",
    "from flax import nnx\n",
    "import grain\n",
    "import humanize\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import kagglehub\n",
    "import optax\n",
    "from orbax import checkpoint as ocp\n",
    "from pathlib import Path\n",
    "import qwix\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# from tunix.models.gemma3 import model as gemma_lib\n",
    "# from tunix.models.gemma3 import params as params_lib\n",
    "from tunix.models.gemma3 import params\n",
    "from tunix.models.gemma3 import model\n",
    "from tunix.rl import rl_cluster as rl_cluster_lib\n",
    "from tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\n",
    "from tunix.rl.rollout import base_rollout\n",
    "from tunix.sft import metrics_logger\n",
    "from datasets import load_dataset\n",
    "from google import genai\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eu_NI9nHTn1k"
   },
   "source": [
    "## Configuration of Hyperparameters\n",
    "\n",
    "The choice of the hyperparameters is tricky because I cant use gridsearch or optuna to optimize them, it would be very expensive. The model size is small, so I though maybe to use a smaller rank, on the other hand the task we mean to train is a big change of policy, so I will keep the rank at 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:23:16.568364Z",
     "iopub.status.busy": "2025-12-02T17:23:16.568077Z",
     "iopub.status.idle": "2025-12-02T17:23:16.575284Z",
     "shell.execute_reply": "2025-12-02T17:23:16.574383Z",
     "shell.execute_reply.started": "2025-12-02T17:23:16.568346Z"
    },
    "id": "ZPPKme47Tn1k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Configs:\n",
    "    \n",
    "    # ====== Data ======\n",
    "    TRAIN_DATA_DIR = \"./data/train\"\n",
    "    TEST_DATA_DIR = \"./data/test\"\n",
    "    TRAIN_FRACTION = 0.8\n",
    "    \n",
    "    # ====== LoRA ======\n",
    "    RANK = 32\n",
    "    ALPHA = 32.0\n",
    "    \n",
    "    # ====== Sharding ======\n",
    "    MESH = [(1, 4), (\"fsdp\", \"tp\")]\n",
    "    \n",
    "    # ====== GRPO ======\n",
    "    # === Generation during GRPO training ===\n",
    "    MAX_PROMPT_LENGTH = 256\n",
    "    TOTAL_GENERATION_STEPS = 512\n",
    "    \n",
    "    # Important to keep a high-ish temperature for varied, diverse responses during\n",
    "    # training.\n",
    "    TEMPERATURE = 0.9\n",
    "    TOP_P = 0.95\n",
    "    TOP_K = 50\n",
    "\n",
    "    # The number of times the policy generates multiple responses for a given prompt within a single training step. \n",
    "    # So basically the number of groups, I will set it to 16. Originally google kept it as 4, but seems to litle for such a task\n",
    "    NUM_GENERATIONS = 16\n",
    "    \n",
    "    # === other GRPO configs ===\n",
    "    # The number of iterations per batch (ùúá in GRPO algo 1).\n",
    "    NUM_ITERATIONS = 1\n",
    "    # The coefficient for the KL divergence penalty (ùõΩ) in the GRPO loss function.\n",
    "    # Important to keep a high enough value for this, otherwise, the KL divergence\n",
    "    # can increase unchecked and the model may overfit or suffer from catastrophical forgetting.\n",
    "    # But using LoRa is that really a risk ? we wont even touch the the original matrix/policy..... lets keep it lower, say 0.04 (they had 0.08)\n",
    "    BETA = 0.04\n",
    "    \n",
    "    # Epsilon value for clipping. Similar to PPO, for\n",
    "    # stable updates.\n",
    "    EPSILON = 0.2\n",
    "    \n",
    "    # ====== Training ======\n",
    "    TRAIN_MICRO_BATCH_SIZE = 4\n",
    "    # Increase `NUM_BATCHES` and `MAX_STEPS` for better results.\n",
    "    NUM_BATCHES = 3738\n",
    "    # Keep `NUM_TEST_BATCHES` low so that evaluation runs quickly. It can be\n",
    "    # increased to a max. of 330 (if batch size is 4).\n",
    "    NUM_TEST_BATCHES = 100\n",
    "    \n",
    "    EVAL_EVERY_N_STEPS = 10  # this doesn't matter if `TRAIN_FRACTION = 1.0`.\n",
    "    NUM_EPOCHS = 2  # each epoch takes around 1.5 hours\n",
    "    \n",
    "    # Number of training steps.\n",
    "    MAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\n",
    "    \n",
    "    # === AdamW, warmup, cosine scheduler ===\n",
    "    LEARNING_RATE = 3e-6\n",
    "    B1 = 0.9\n",
    "    B2 = 0.99\n",
    "    WEIGHT_DECAY = 0.1\n",
    "    # == Cosine decay with warmup scheduler ==\n",
    "    # Linearly increase learning rate from 0. to 5e-6 in the first 10% training\n",
    "    # steps, and then gradually decrease the learning rate to 0 using cosine\n",
    "    # scheduler.\n",
    "    WARMUP_STEPS = 0.1 * MAX_STEPS\n",
    "    # == Grad clipping ==\n",
    "    # Grad clipping to prevent large gradients. Found this\n",
    "    # important to keep KL divergence in check.\n",
    "    MAX_GRAD_NORM = 0.1\n",
    "    \n",
    "    # Checkpoint saving\n",
    "    INTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\n",
    "    CKPT_DIR = \"/tmp/content/ckpts/\"\n",
    "    SAVE_INTERVAL_STEPS = 500\n",
    "    MAX_TO_KEEP = 4\n",
    "\n",
    "    # ====== Inference ======\n",
    "    GENERATION_CONFIGS = {\n",
    "        # greedy search\n",
    "        \"greedy\": {\"temperature\": 1e-3, \"top_k\": 3, \"top_p\": 1.0},\n",
    "        # some randomness\n",
    "        \"standard\": {\"temperature\": 0.8, \"top_k\": 50, \"top_p\": 0.95},\n",
    "        # liberal\n",
    "        \"liberal\": {\"temperature\": 0.95, \"top_k\": 2000, \"top_p\": 1.0},\n",
    "    }\n",
    "\n",
    "    # Prompt stuff\n",
    "    #let's define some special tokens. We instruct the model to first reason\n",
    "    #between the `<reasoning>` and `</reasoning>` tokens. After\n",
    "    #reasoning, we expect it to provide the answer between the `<answer>` and\n",
    "    #`</answer>` tokens.\n",
    "    reasoning_start = \"<reasoning>\"\n",
    "    reasoning_end = \"</reasoning>\"\n",
    "    solution_start = \"<answer>\"\n",
    "    solution_end = \"</answer>\"\n",
    "    \n",
    "    \n",
    "    SYSTEM_PROMPT = f\"\"\"You are given a problem. Think about the problem and \\\n",
    "    provide your reasoning. Place it between {reasoning_start} and \\\n",
    "    {reasoning_end}. Then, provide the final answer (i.e., just one numerical \\\n",
    "    value) between {solution_start} and {solution_end}.\"\"\"\n",
    "\n",
    "    JUDGE_PROMPT = \"\"\"You are a reasoning judge. You will get a user request and a candidate response.  \n",
    "                    Your job: assign rewards based on **reasoning process**, not just correctness of answer.  \n",
    "                    \n",
    "                    Three criteria for reward:  \n",
    "                    1. **Gathering and organizing key information**: how well candidate identifies the relevant facts and constraints from the problem.  \n",
    "                    2. **Analyzing possible action paths**: how well candidate explores different strategies, considers consequences, iterates logically.  \n",
    "                    3. **Choosing an action path and providing an answer**: how well candidate uses their analysis to propose a solution, even if incomplete, and iterates if needed.  \n",
    "                    \n",
    "                    Important instructions:  \n",
    "                    - Focus on **process** over final answer correctness.  \n",
    "                    - Correct answers matter less; reward structured, thoughtful reasoning more.  \n",
    "                    - If candidate iterates, considers multiple options, refines solution ‚Üí higher reward.  \n",
    "                    - Output **JSON only** matching schema exactly:  \n",
    "                      ```json\n",
    "                      {\n",
    "                        \"reward_info_gathering_and_organization\": <int 0-10>,\n",
    "                        \"reward_analyse_possible_action_paths\": <int 0-10>,\n",
    "                        \"reward_choose_an_action_path_and_return_an_answer_related_to_it\": <int 0-10>\n",
    "                      }\n",
    "                    \"\"\"\n",
    "    \n",
    "    TEMPLATE = \"\"\"<start_of_turn>user\n",
    "    {system_prompt}\n",
    "    \n",
    "    {question}<end_of_turn>\n",
    "    <start_of_turn>model\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ngjtE-63Tn1k"
   },
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:23:17.835161Z",
     "iopub.status.busy": "2025-12-02T17:23:17.834890Z",
     "iopub.status.idle": "2025-12-02T17:23:17.838915Z",
     "shell.execute_reply": "2025-12-02T17:23:17.838000Z",
     "shell.execute_reply.started": "2025-12-02T17:23:17.835141Z"
    },
    "id": "wjMFOr7aTn1k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def show_hbm_usage():\n",
    "  \"\"\"Displays memory usage per device.\"\"\"\n",
    "  fmt_size = functools.partial(humanize.naturalsize, binary=True)\n",
    "\n",
    "  for d in jax.local_devices():\n",
    "    stats = d.memory_stats()\n",
    "    used = stats[\"bytes_in_use\"]\n",
    "    limit = stats[\"bytes_limit\"]\n",
    "    print(f\"Using {fmt_size(used)} / {fmt_size(limit)} ({used/limit:%}) on {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BtpYMlaTn1k"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WASP9N5JTn1k"
   },
   "source": [
    "#### I use OpenAI's [GSM8K dataset](https://huggingface.co/datasets/openai/gsm8k), which comprises grade school math word problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:23:20.367877Z",
     "iopub.status.busy": "2025-12-02T17:23:20.367618Z",
     "iopub.status.idle": "2025-12-02T17:23:20.376364Z",
     "shell.execute_reply": "2025-12-02T17:23:20.375350Z",
     "shell.execute_reply.started": "2025-12-02T17:23:20.367862Z"
    },
    "id": "gTGjcSMNTn1k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_hash_answer(text: str) -> str | None:\n",
    "  if \"####\" not in text:\n",
    "    return None\n",
    "  return text.split(\"####\")[1].strip()\n",
    "\n",
    "\n",
    "def _load_from_tfds(data_dir: str, split: str):\n",
    "  import tensorflow_datasets.text.gsm8k\n",
    "  return tfds.data_source(\n",
    "      \"gsm8k\",\n",
    "      split=split,\n",
    "      data_dir=data_dir,\n",
    "      builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
    "      download=True,\n",
    "  )\n",
    "\n",
    "\n",
    "def download_kaggle_dataset(target_dir=\"./data/gsm8k\"):\n",
    "  os.makedirs(target_dir, exist_ok=True)\n",
    "  src = kagglehub.dataset_download(\"thedevastator/grade-school-math-8k-q-a\")\n",
    "  src = Path(src)\n",
    "  dst = Path(target_dir)\n",
    "\n",
    "  for csv_file in src.glob(\"*.csv\"):  # match all CSV files\n",
    "    shutil.copy2(csv_file, dst / csv_file.name)\n",
    "    print(f\"Copied {csv_file.name} ‚Üí {dst/csv_file.name}\")\n",
    "  return target_dir\n",
    "\n",
    "\n",
    "def get_dataset(data_dir, split=\"train\", source=\"tfds\") -> grain.MapDataset:\n",
    "  # Download data\n",
    "  if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "  if source == \"tfds\":\n",
    "    import tensorflow_datasets.text.gsm8k\n",
    "    data = tfds.data_source(\n",
    "        \"gsm8k\",\n",
    "        split=split,\n",
    "        data_dir=data_dir,\n",
    "        builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "  elif source == \"kaggle\":\n",
    "    kaggle_dir = download_kaggle_dataset(data_dir)\n",
    "    file_name = \"main_\" + split + \".csv\"\n",
    "    csv_path = os.path.join(kaggle_dir, file_name)  # adjust filename if needed\n",
    "\n",
    "    data = []\n",
    "    with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "      reader = csv.DictReader(csvfile)\n",
    "      for row in reader:\n",
    "        data.append({\n",
    "            \"question\": row[\"question\"],\n",
    "            \"answer\": row[\"answer\"],\n",
    "        })\n",
    "\n",
    "  elif source == \"huggingface\":    \n",
    "    os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "    data = load_dataset(\"gsm8k\", \"main\", split=split)\n",
    "      \n",
    "  else:\n",
    "    raise ValueError(f\"Unknown source: {source}\")\n",
    "\n",
    "  def _as_text(v):\n",
    "    return v if isinstance(v, str) else v.decode(\"utf-8\")\n",
    "\n",
    "  dataset = (\n",
    "      grain.MapDataset.source(data)\n",
    "      .shuffle(seed=42)\n",
    "      .map(\n",
    "          lambda x: {\n",
    "              # passed to model forward pass\n",
    "              \"prompts\": Configs.TEMPLATE.format(\n",
    "                  system_prompt=Configs.SYSTEM_PROMPT,\n",
    "                  question=_as_text(x[\"question\"]),\n",
    "              ),\n",
    "              # passed to reward functions\n",
    "              \"question\": _as_text(x[\"question\"]),\n",
    "              # passed to reward functions\n",
    "              \"answer\": extract_hash_answer(_as_text(x[\"answer\"])),\n",
    "          }\n",
    "      )\n",
    "  )\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDwobMu_okwv"
   },
   "source": [
    "## I split the dataset set into train and test sets as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:23:26.754197Z",
     "iopub.status.busy": "2025-12-02T17:23:26.753903Z",
     "iopub.status.idle": "2025-12-02T17:23:31.414707Z",
     "shell.execute_reply": "2025-12-02T17:23:31.413796Z",
     "shell.execute_reply.started": "2025-12-02T17:23:26.754178Z"
    },
    "id": "KXhOL6GyTn1k",
    "outputId": "5e15f893-33eb-42e9-f4bd-20be01f2314a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# source = input(\"Choose data source [tfds/kaggle]: \").strip().lower()\n",
    "Configs.Source = \"huggingface\"\n",
    "\n",
    "if Configs.Source not in (\"tfds\", \"kaggle\", \"huggingface\"):\n",
    "  print(\"Invalid choice. Defaulting to 'tfds'.\")\n",
    "  source = \"\"\n",
    "\n",
    "print(f\"Using data source: {Configs.Source}\")\n",
    "\n",
    "dataset = get_dataset(Configs.TRAIN_DATA_DIR, \"train\", Configs.Source).batch(Configs.TRAIN_MICRO_BATCH_SIZE)[\n",
    "    :Configs.NUM_BATCHES\n",
    "]\n",
    "\n",
    "if Configs.TRAIN_FRACTION == 1.0:\n",
    "  train_dataset = dataset.repeat(Configs.NUM_EPOCHS)\n",
    "  val_dataset = None\n",
    "else:\n",
    "  train_dataset = dataset[: int(len(dataset) * Configs.TRAIN_FRACTION)]\n",
    "  train_dataset = train_dataset.repeat(Configs.NUM_EPOCHS)\n",
    "\n",
    "  val_dataset = dataset[int(len(dataset) * Configs.TRAIN_FRACTION) :].repeat(Configs.NUM_EPOCHS)\n",
    "\n",
    "test_dataset = get_dataset(Configs.TEST_DATA_DIR, \"test\", Configs.Source).batch(Configs.TRAIN_MICRO_BATCH_SIZE)[\n",
    "    :Configs.NUM_TEST_BATCHES\n",
    "]\n",
    "\n",
    "dataset_lengths = (\n",
    "    len(train_dataset),\n",
    "    len(val_dataset) if val_dataset is not None else 0,\n",
    "    len(test_dataset),\n",
    ")\n",
    "print(f\"dataset contains {dataset_lengths} of batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7n8L0VzTn1k"
   },
   "source": [
    "## Let's see how one batch of the training dataset looks like!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:23:31.415652Z",
     "iopub.status.busy": "2025-12-02T17:23:31.415217Z",
     "iopub.status.idle": "2025-12-02T17:23:31.420844Z",
     "shell.execute_reply": "2025-12-02T17:23:31.420123Z",
     "shell.execute_reply.started": "2025-12-02T17:23:31.415637Z"
    },
    "id": "5TF-wNQ2Tn1k",
    "outputId": "367cd3ef-9b1c-469d-b50c-71887b040e87",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for ele in train_dataset[:1]:\n",
    "  pprint(ele)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZxBR7Y_Tn1k"
   },
   "source": [
    "## Load the policy model and the reference model\n",
    "\n",
    "The policy model is the model which is actually trained and whose weights are\n",
    "updated. The reference model is the model with which we compute KL divergence.\n",
    "\n",
    "Typically, the reference model is the base model, and the policy model is the\n",
    "same base model, but with the LoRA parameters added to it. Only the LoRA parameters are updated. So (if you are familiar with the deepseek notation [DeepSeek](https://arxiv.org/pdf/2501.12948) ) Pi_old and Pi_theta will have the full model + the LoRa weights and the Pi_ref is just the original model. \n",
    "\n",
    "Note: We perform full precision (fp32) training. You can, however, leverage\n",
    "Qwix for QAT.\n",
    "\n",
    "To load the model, you need to be on [Kaggle](https://www.kaggle.com/) and need\n",
    "to have agreed to the Gemma license\n",
    "[here](https://www.kaggle.com/models/google/gemma/flax/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:23:32.388143Z",
     "iopub.status.busy": "2025-12-02T17:23:32.387890Z",
     "iopub.status.idle": "2025-12-02T17:23:32.524791Z",
     "shell.execute_reply": "2025-12-02T17:23:32.524021Z",
     "shell.execute_reply.started": "2025-12-02T17:23:32.388092Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"KAGGLE_KEY\"] = UserSecretsClient().get_secret(\"KAGGLE_KEY\")\n",
    "os.environ[\"KAGGLE_USERNAME\"] = UserSecretsClient().get_secret(\"KAGGLE_USERNAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:23:33.262674Z",
     "iopub.status.busy": "2025-12-02T17:23:33.262460Z",
     "iopub.status.idle": "2025-12-02T17:23:33.265650Z",
     "shell.execute_reply": "2025-12-02T17:23:33.264895Z",
     "shell.execute_reply.started": "2025-12-02T17:23:33.262659Z"
    },
    "id": "3GfLHHVYHHKO",
    "outputId": "3290c9e8-2362-44a1-93bf-9a5caa7d201f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Log in\n",
    "if \"KAGGLE_USERNAME\" not in os.environ.keys() or \"KAGGLE_KEY\" not in os.environ.keys():\n",
    "  kagglehub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAghcsT_Pmv_"
   },
   "source": [
    "This code snippet serves as a workaround to re-save the pre-trained model checkpoint from Kaggle into a local format that is compatible with the [Flax NNX](https://flax.readthedocs.io/en/stable/why.html) library. Because the original checkpoint has parameter names and tensor structures that don't match the target NNX model architecture, it cannot be loaded directly.\n",
    "\n",
    "So let's first load the original weights into a temporary model instance, then extract and re-save the model's state into a new, properly formatted local checkpoint, which can then be successfully loaded by the final sharded NNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cIFAxgVOTn1k",
    "outputId": "0235a0e1-9f7d-428c-c16e-0bc9e49c2f2f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm /tmp/content/intermediate_ckpt/* -rf\n",
    "\n",
    "!rm /tmp/content/ckpts/* -rf\n",
    "\n",
    "model_family = \"gemma3\"\n",
    "if model_family == \"gemma3\":\n",
    "  Configs.MODEL_CP_PATH = params.GEMMA3_1B_IT\n",
    "  config = model.ModelConfig.gemma3_1b()\n",
    "  gemma = params.create_model_from_checkpoint(Configs.MODEL_CP_PATH, config)\n",
    "  tokenizer = params.create_tokenizer()\n",
    "\n",
    "  checkpointer = ocp.StandardCheckpointer()\n",
    "  ## nnx.split splits the single module instance into two separate objects:\n",
    "  ##1 - Variables/state (params, batch stats, rngs, etc.)\n",
    "  ##2 - The pure callable module (the ‚Äúfunction‚Äù part)\n",
    "  _, state = nnx.split(gemma)\n",
    "  checkpointer.save(os.path.join(Configs.INTERMEDIATE_CKPT_DIR, \"state\"), state)\n",
    "  checkpointer.wait_until_finished()\n",
    "  # Delete the intermediate model to save memory.\n",
    "  del params\n",
    "  del gemma\n",
    "  del state\n",
    "  gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpgXONuORkkq"
   },
   "source": [
    "### Model Loading and LoRA Application\n",
    "\n",
    "These two functions work together to load a base model from a checkpoint and apply a LoRA (Low-Rank Adaptation) layer to it.\n",
    "\n",
    "* `get_ref_model`: Loads the complete Gemma model from a specified checkpoint path. It uses **JAX sharding** to distribute the model parameters across multiple devices (usually TPU cores).\n",
    "* \n",
    "* `get_lora_model`: Takes the base model and applies LoRA layers to it. It uses a `LoraProvider` to select specific layers/transformer-sub-modules (like attention and MLP layers) to be adapted. The resulting LoRA-infused model is then sharded and updated to ensure it's ready for distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:26:02.437590Z",
     "iopub.status.busy": "2025-12-02T17:26:02.437250Z",
     "iopub.status.idle": "2025-12-02T17:26:02.444604Z",
     "shell.execute_reply": "2025-12-02T17:26:02.443527Z",
     "shell.execute_reply.started": "2025-12-02T17:26:02.437569Z"
    },
    "id": "m2KD-nmbTn1k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tunix.models.gemma3 import params\n",
    "\n",
    "def get_gemma_ref_model(ckpt_path):\n",
    "  # build device mesh using config (e.g. fsdp/tp layout)\n",
    "  mesh = jax.make_mesh(*Configs.MESH)\n",
    "\n",
    "  # pick model config for Gemma 3 1B\n",
    "  model_config = model.ModelConfig.gemma3_1b()\n",
    "\n",
    "  # build abstract model (only shapes, no real weights)\n",
    "  abs_gemma: nnx.Module = nnx.eval_shape(\n",
    "      lambda: params.create_model_from_checkpoint(Configs.MEL_CP_PATH, config)\n",
    "  )\n",
    "\n",
    "  # extract state tree from model (params/buffers)\n",
    "  abs_state = nnx.state(abs_gemma)\n",
    "\n",
    "  # replace each param with shape-dtype struct and attach sharding info\n",
    "  abs_state = jax.tree.map(\n",
    "      lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n",
    "      abs_state,\n",
    "      nnx.get_named_sharding(abs_state, mesh),\n",
    "  )\n",
    "\n",
    "  # create checkpointer for loading weights\n",
    "  checkpointer = ocp.StandardCheckpointer()\n",
    "\n",
    "  # load real params from checkpoint into abstract state\n",
    "  restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n",
    "\n",
    "  # split model into graph (structure) and empty param tree\n",
    "  graph_def, _ = nnx.split(abs_gemma)\n",
    "\n",
    "  # merge graph structure with restored real params to make runnable model\n",
    "  gemma = nnx.merge(graph_def, restored_params)\n",
    "\n",
    "  # return fully built model + mesh + config\n",
    "  return gemma, mesh, model_config\n",
    "\n",
    "\n",
    "def get_lora_model(base_model, mesh):\n",
    "    # define function take base model and mesh to add LoRA\n",
    "\n",
    "    lora_provider = qwix.LoraProvider(\n",
    "        # create LoRA provider, target modules by regex\n",
    "        module_path=(\n",
    "            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n",
    "            \".*attn_vec_einsum\"\n",
    "        ),\n",
    "        rank=Configs.RANK,   # LoRA low-rank size\n",
    "        alpha=Configs.ALPHA, # LoRA scaling factor\n",
    "    )\n",
    "\n",
    "    model_input = base_model.get_model_input()\n",
    "    # extract model input info (shapes/dtypes) for LoRA injection\n",
    "\n",
    "    lora_model = qwix.apply_lora_to_model(\n",
    "        base_model, lora_provider, **model_input\n",
    "    )\n",
    "    # apply LoRA adapters, return new model with LoRA layers attached\n",
    "\n",
    "    with mesh:\n",
    "        state = nnx.state(lora_model)\n",
    "        # extract PyTree of model parameters + buffers\n",
    "\n",
    "        pspecs = nnx.get_partition_spec(state)\n",
    "        # get sharding specification for each param on the mesh\n",
    "\n",
    "        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "        # attach sharding constraints, ensures correct device placement\n",
    "\n",
    "        nnx.update(lora_model, sharded_state)\n",
    "        # update model with sharded weights, ready for forward/backward\n",
    "\n",
    "    return lora_model\n",
    "    # return LoRA-augmented, sharded model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:26:03.208604Z",
     "iopub.status.busy": "2025-12-02T17:26:03.208362Z",
     "iopub.status.idle": "2025-12-02T17:26:03.212996Z",
     "shell.execute_reply": "2025-12-02T17:26:03.212062Z",
     "shell.execute_reply.started": "2025-12-02T17:26:03.208585Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import flax\n",
    "flax.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgBALRieR6aY"
   },
   "source": [
    "## Load the reference and policy Gemma models using the Flax NNX library and display their structures. \n",
    "\n",
    "### Remember that the reference Model will stay untouched, it is only used to compute the KL Divergence at each time step, while the policy model uses LoRa, meaning it is the model whose weights will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:26:04.227012Z",
     "iopub.status.busy": "2025-12-02T17:26:04.226791Z",
     "iopub.status.idle": "2025-12-02T17:26:04.623682Z",
     "shell.execute_reply": "2025-12-02T17:26:04.622461Z",
     "shell.execute_reply.started": "2025-12-02T17:26:04.226997Z"
    },
    "id": "kSdZ7aGhTn1k",
    "outputId": "a536819f-dd5f-4e29-8ebe-09234960c114",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Reference model\n",
    "if model_family == \"gemma3\":\n",
    "  ref_model, mesh, model_config = get_gemma_ref_model(\n",
    "      ckpt_path=os.path.join(Configs.INTERMEDIATE_CKPT_DIR, \"state\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4i3CfJ1gTn1k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Policy model\n",
    "lora_policy = get_lora_model(ref_model, mesh=mesh)\n",
    "# nnx.display(lora_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLzR1tJfTn1k"
   },
   "source": [
    "## Define reward functions\n",
    "\n",
    "The original Tunix team defined four reward functions:\n",
    "\n",
    "- reward if the format of the output exactly matches the instruction given in\n",
    "`TEMPLATE`;\n",
    "- reward if the format of the output approximately matches the instruction given\n",
    "in `TEMPLATE`;\n",
    "- reward if the answer is correct/partially correct;\n",
    "- Sometimes, the text between `<answer>`, `</answer>` might not be one\n",
    "  number. So, we extract the number, and reward the model if the answer is correct.\n",
    "\n",
    "The reward functions are inspired from\n",
    "[here](https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb), but they also seem to be somewhat in line with the [deepseek](https://arxiv.org/pdf/2501.12948) paper (section 2.2.2).\n",
    "\n",
    "Far√° sentido beneficiar assim tanto o formato ? talvez, mas tenho de verificar se o formato √© avaliado a dobrar, pq podem usar a primeira e a segunda reward funtion, o que significa que o total de recompensa pelo formato completo √© de 7 valores (parece me alto). De resto √© assim que devemos ensinar o modelo a pensar a um n√≠vel mais profundo? verificar que nos deixou tokens a dizer que houve reasoning ou n√£o ? ser√° que √© assim que vamos aproximar o racioc√≠nio da m√°quina ao nosso ?\n",
    "\n",
    "## A parte das reward funtions √© a mais importante do treino, √© o que vai definir o objectivo do treino, por isso devemos recompensar racioc√≠nio que se assemlhe ao nosso e n√£o apenas formato e respostas certas. \n",
    "\n",
    "### Como raciocinamos n√≥s ? \n",
    "Problema : Tens tr√™s interruptores fora de uma sala fechada ‚Äî A, B e C ‚Äî e dentro da sala h√° tr√™s l√¢mpadas (cada interruptor controla exactamente uma l√¢mpada). Podes mexer nos interruptores o quanto quiseres, mas s√≥ podes entrar na sala uma vez (depois de entrares n√£o podes voltar a mexer nos interruptores). Como descobres qual interruptor controla qual l√¢mpada?\n",
    "\n",
    "Linha de pensamento : \n",
    "##### 1 - recolher e organizar a informa√ß√£o chave \n",
    "(Cen√°rio : 3 interruptores ligados cada um a uma l√¢mpada apenas. L√¢mpadas dentro de uma sala fechada. Objectivo : Descobrir que interruptor se liga a qual l√¢mpada. Constragimento : s√≥ posso entrar na sala uma vez e n√£o posso mexer nos interruptores depois de o fazer.\n",
    "##### 2 - Analisar caminhos de a√ß√£o poss√≠veis com base nessa info e no meu conhecimento\n",
    "( Cen√°rio de a√ß√£o 1 : ligar todos os interruptores, leva a uma inconlus√£o porque continua a ser imposs√≠vel associar um interruptor a uma l√¢mpada, Cen√°rio 2 : ligar apenas uma l√¢mpada; tb ser√° inconclusivo pois s√≥ me d√° info sobre uma das associa√ß√µes . Cen√°rio 3 : ligar duas l√™mpadas, idem... etc)\n",
    "##### 3 - Com base na an√°lise providenciar uma resposta final. \n",
    "Neste caso, n√£o sei como o fazer, isto √© uma conclus√£o v√°lida que me vai reencaminhar de novo para o ponto 2, procurar novos cen√°rio de a√ß√£o. Se eu fosse uma m√°quina continuaria a iterar at√© acabar.\n",
    "\n",
    "### Como atribuir recompensas quando os crit√©rios s√£o mais vagos ?\n",
    "\n",
    "Seria muito mais interessante avaliar as respostas e atribuir uma recompensa com base nestes crit√©rios, ou seja, ver se o LLM recolheu e organizou info chave, analisou caminhos de a√ß√£o poss√≠veis com base na info chave e no seu conhecimento e por fim se tentou chegar a uma resposta (iterando quando n√£o a achou). \n",
    "\n",
    "Aqui, se calhar, at√© deixaria a resposta certa com um peso muito mais baixo, pouco me interessa que ele acerte as perguntas durante o treino, possivelmente j√° as viu, quero √© dar √™nfase ao processo de reflex√£o. Embora a resposta certa deva estar presente na Reward Funtion, porque assume-se que um bom processo de pensamento leve mais vezes a uma resposta certa.\n",
    "\n",
    "Se calhar a forma mais eficiente de atribuir uma recompensa neste caso √© mesmo usando um LLM as a judge, porque √© imposs√≠vel avaliar isto se outra forma; a n√£o ser que construamos um classificador que o fa√ßa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Rewards(BaseModel):\n",
    "    reward_info_gathering_and_organization: int = Field(description=\"the reward associated with the ability to gather info and organize it\")\n",
    "    reward_analyse_possible_action_paths: int = Field(description=\"the reward associated with the analysis possible action paths\")\n",
    "    reward_choose_an_action_path_and_return_an_answer_related_to_it: int = Field(description=\"The reward associated with choosing an action path and the returning of an answer related to it\")\n",
    "    \n",
    "class Gemini_judge:\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.client = genai.Client(api_key=\"GOOGLE_YOUR_API_KEY\")\n",
    "\n",
    "        self.major_judge_prompt = Configs.JUDGE_PROMPT\n",
    "\n",
    "\n",
    "    \n",
    "    def llm_as_judge(self,input_prompt: str):\n",
    "    \n",
    "        response = self.client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=input_prompt,\n",
    "            config={\n",
    "                \"response_mime_type\": \"application/json\",\n",
    "                \"response_json_schema\": Rewards.model_json_schema(),\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        rewards = Rewards.model_validate_json(response.text)\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7Beft8wTn1k",
    "outputId": "a0ba4233-562d-485d-b9ba-2f22cf2785b4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# First off, let's define a RegEx for checking whether the format matches.\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\n",
    "    rf\"{Configs.reasoning_start}.+?{Configs.reasoning_end}.*?\"\n",
    "    rf\"{Configs.solution_start}(.+?){Configs.solution_end}\"\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags=re.MULTILINE | re.DOTALL,\n",
    ")\n",
    "\n",
    "match_format.search(\n",
    "    f\"{Configs.reasoning_start}Let me\"\n",
    "    f\" think!{Configs.reasoning_end}{Configs.solution_start}2{Configs.solution_end}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fe1rF15zTn1k"
   },
   "source": [
    "Give the model a reward of 3 points if the format matches exactly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fhQ6pY2Tn1k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def match_format_exactly(prompts, completions, **kwargs):\n",
    "  return [\n",
    "      0 if match_format.search(response) is None else 3.0\n",
    "      for response in completions\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWdAdUHuTn1k"
   },
   "source": [
    "Reward the model if the format of the output matches partially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uOhO4f3-Tn1k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def match_format_approximately(prompts, completions, **kwargs):\n",
    "  scores = []\n",
    "\n",
    "  for completion in completions:\n",
    "    score = 0\n",
    "    response = completion\n",
    "    # Count how many keywords are seen - we penalize if too many!\n",
    "    # If we see 1, then plus some points!\n",
    "    score += 0.5 if response.count(Configs.reasoning_start) == 1 else -0.5\n",
    "    score += 0.5 if response.count(Configs.reasoning_end) == 1 else -0.5\n",
    "    score += 0.5 if response.count(Configs.solution_start) == 1 else -0.5\n",
    "    score += 0.5 if response.count(Configs.solution_end) == 1 else -0.5\n",
    "    scores.append(score)\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2fNZDgTTn1k"
   },
   "source": [
    "### Reward the model if the answer is correct. A reward is also given if the answer does not match exactly, i.e., based on how close the answer is to the correct value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8zcWsmhTn1k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "  responses = completions\n",
    "\n",
    "  extracted_responses = [\n",
    "      guess.group(1) if (guess := match_format.search(r)) is not None else None\n",
    "      for r in responses\n",
    "  ]\n",
    "\n",
    "  scores = []\n",
    "  assert len(extracted_responses) == len(\n",
    "      answer\n",
    "  ), f\"{extracted_responses} and {answer} have mismatching length\"\n",
    "  for guess, true_answer in zip(extracted_responses, answer):\n",
    "    score = 0\n",
    "    if guess is None:\n",
    "      scores.append(0)\n",
    "      continue\n",
    "    # Correct answer gets 3 points!\n",
    "    if guess == true_answer:\n",
    "      score += 3.0\n",
    "    # Match if spaces are seen\n",
    "    elif guess.strip() == true_answer.strip():\n",
    "      score += 1.5\n",
    "    else:\n",
    "      # We also reward it if the answer is close via ratios!\n",
    "      # Ie if the answer is within some range, reward it!\n",
    "      try:\n",
    "        ratio = float(guess) / float(true_answer)\n",
    "        if ratio >= 0.9 and ratio <= 1.1:\n",
    "          score += 0.5\n",
    "        elif ratio >= 0.8 and ratio <= 1.2:\n",
    "          score += 0.25\n",
    "        else:\n",
    "          score -= 1.0  # Penalize wrong answers\n",
    "      except:\n",
    "        score -= 0.5  # Penalize\n",
    "    scores.append(score)\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIpOVv78Tn1k"
   },
   "source": [
    "Sometimes, the text between `<answer>` and `</answer>` might not be one\n",
    "number; it can be a sentence. So, we extract the number and compare the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXvRtbk8Tn1k",
    "outputId": "1ab45f0e-d04a-455c-a046-7c7a1ec6ee22",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "match_numbers = re.compile(\n",
    "    rf\"{Configs.solution_start}.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL\n",
    ")\n",
    "match_numbers.findall(f\"{Configs.solution_start}  0.34  {Configs.solution_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxZQAFKOTn1k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "  question = kwargs[\"question\"]\n",
    "  responses = completions\n",
    "\n",
    "  extracted_responses = [\n",
    "      guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n",
    "      for r in responses\n",
    "  ]\n",
    "\n",
    "  scores = []\n",
    "  print(\"START ============================\")\n",
    "  print(f\"Question: {question[0]}\")\n",
    "  print(f\"Answer: {answer[0]}\")\n",
    "  print(f\"Response: {responses[0]}\")\n",
    "  print(f\"Extracted: {extracted_responses[0]}\")\n",
    "  print(\"END ==============================\")\n",
    "  for guess, true_answer in zip(extracted_responses, answer):\n",
    "    if guess is None:\n",
    "      scores.append(0)\n",
    "      continue\n",
    "    # Convert to numbers\n",
    "    try:\n",
    "      true_answer = float(true_answer.strip())\n",
    "      guess = float(guess.strip())\n",
    "      scores.append(1.5 if guess == true_answer else 0.0)\n",
    "    except:\n",
    "      scores.append(0)\n",
    "      continue\n",
    "  return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaiYMJxFTn1k"
   },
   "source": [
    "## Evaluate\n",
    "\n",
    "\n",
    "Before we train the model, let's evaluate the model on the test set so we can\n",
    "see the improvement post training.\n",
    "\n",
    "We evaluate it in two ways:\n",
    "\n",
    "**Quantitative**\n",
    "\n",
    "* **Answer Accuracy**: percentage of samples for which the model predicts the\n",
    "correct final numerical answer  \n",
    "* **Answer (Partial) Accuracy**: percentage of samples for which the model\n",
    "predicts a final numerical answer such that the \\`model answer / answer\\`\n",
    "ratio lies between 0.9 and 1.1.  \n",
    "* **Format Accuracy**: percentage of samples for which the model outputs the\n",
    "correct format, i.e., reasoning between the reasoning special tokens, and the\n",
    "final answer between the \\`\\<start\\_answer\\>\\`, \\`\\<end\\_answer\\>\\` tokens.\n",
    "\n",
    "**Qualitative**\n",
    "\n",
    "We'll also print outputs for a few given questions so that we can compare the generated output later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAaZ7NjBx99P"
   },
   "source": [
    "We define a helper function to generate an answer, given a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_k58bOicUHJy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate(\n",
    "    question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None\n",
    "):\n",
    "  \"\"\"Given prompt, generates text.\"\"\"\n",
    "\n",
    "  if isinstance(question, str):\n",
    "    input_batch = [\n",
    "        Configs.TEMPLATE.format(\n",
    "            system_prompt=Configs.SYSTEM_PROMPT,\n",
    "            question=question,\n",
    "        ),\n",
    "    ]\n",
    "  else:\n",
    "    input_batch = [\n",
    "        Configs.TEMPLATE.format(\n",
    "            system_prompt=Configs.SYSTEM_PROMPT,\n",
    "            question=q,\n",
    "        )\n",
    "        for q in question\n",
    "    ]\n",
    "\n",
    "  out_data = sampler(\n",
    "      input_strings=input_batch,\n",
    "      max_generation_steps=768,\n",
    "      temperature=temperature,\n",
    "      top_k=top_k,\n",
    "      top_p=top_p,\n",
    "      echo=False,\n",
    "      seed=seed if seed is not None else None,\n",
    "      eos_tokens=[1,106],\n",
    "  )\n",
    "\n",
    "  output = out_data.text\n",
    "  if isinstance(question, str):\n",
    "    return output[0]\n",
    "  return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNoa5je7yJOt"
   },
   "source": [
    "Another helper function for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJo2nuKB-wlw",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    dataset,\n",
    "    sampler,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_passes=1,\n",
    "    corr_lst=False,\n",
    "    make_lst=False,\n",
    "):\n",
    "  \"\"\"Computes accuracy and percentage of outputs matching the format.\"\"\"\n",
    "\n",
    "  response_lst = []\n",
    "  corr = 0\n",
    "  partially_corr = 0\n",
    "  corr_format = 0\n",
    "  total = 0\n",
    "\n",
    "  for batch in tqdm(dataset):\n",
    "    expected_answers = batch[\"answer\"]\n",
    "    questions = batch[\"question\"]\n",
    "\n",
    "    multiple_call_responses = [[] for _ in range(len(questions))]\n",
    "    for p in range(num_passes):\n",
    "      responses = generate(\n",
    "          questions, sampler, temperature, top_k, top_p, seed=p\n",
    "      )\n",
    "      for idx, response in enumerate(responses):\n",
    "        multiple_call_responses[idx].append(response)\n",
    "\n",
    "    for question, multiple_call_response, expected_answer in zip(\n",
    "        questions, multiple_call_responses, expected_answers\n",
    "    ):\n",
    "      # check answer\n",
    "      corr_ctr_per_question = 0\n",
    "      partially_corr_per_question = 0\n",
    "      corr_format_per_question = 0\n",
    "      for response in multiple_call_response:\n",
    "        extracted_response = (\n",
    "            guess.group(1)\n",
    "            if (guess := match_numbers.search(response)) is not None\n",
    "            else \"-1000000\"\n",
    "        )\n",
    "        try:\n",
    "          if float(extracted_response.strip()) == float(expected_answer.strip()):\n",
    "            corr_ctr_per_question += 1\n",
    "\n",
    "          ratio = float(extracted_response.strip()) / float(expected_answer.strip())\n",
    "          if ratio >= 0.9 and ratio <= 1.1:\n",
    "            partially_corr_per_question += 1\n",
    "        except:\n",
    "          print(\"SKIPPED\")\n",
    "\n",
    "        # check format\n",
    "        if match_format.search(response) is not None:\n",
    "          corr_format_per_question += 1\n",
    "\n",
    "        if (\n",
    "            corr_ctr_per_question > 0\n",
    "            and partially_corr_per_question > 0\n",
    "            and corr_format_per_question > 0\n",
    "        ):\n",
    "          break\n",
    "\n",
    "      if corr_ctr_per_question > 0:\n",
    "        corr += 1\n",
    "        if corr_lst and make_lst:\n",
    "          response_lst.append((question, expected_answer, multiple_call_response))\n",
    "      else:\n",
    "        if not corr_lst and make_lst:\n",
    "          response_lst.append((question, expected_answer, multiple_call_response))\n",
    "      if partially_corr_per_question > 0:\n",
    "        partially_corr += 1\n",
    "      if corr_format_per_question > 0:\n",
    "        corr_format += 1\n",
    "\n",
    "      total += 1\n",
    "      if total % 10 == 0:\n",
    "        print(\n",
    "            f\"===> {corr=}, {total=}, {corr / total * 100=}, \"\n",
    "            f\"{partially_corr / total * 100=}, {corr_format / total * 100=}\"\n",
    "        )\n",
    "\n",
    "  to_return = (\n",
    "      corr,\n",
    "      total,\n",
    "      corr / total * 100,\n",
    "      partially_corr / total * 100,\n",
    "      corr_format / total * 100,\n",
    "  )\n",
    "  if make_lst:\n",
    "    return to_return, response_lst\n",
    "  return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZMO-KflTn1k",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sampler = sampler_lib.Sampler(\n",
    "    transformer=lora_policy,\n",
    "    tokenizer=tokenizer,\n",
    "    cache_config=sampler_lib.CacheConfig(\n",
    "        cache_size=Configs.MAX_PROMPT_LENGTH + Configs.TOTAL_GENERATION_STEPS + 256,\n",
    "        num_layers=model_config.num_layers,\n",
    "        num_kv_heads=model_config.num_kv_heads,\n",
    "        head_dim=model_config.head_dim,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOAQe06DyVlQ"
   },
   "source": [
    "Now let's see how the original model does on the test set. You can see the percentages of the mode outputs that are fully correct, partially correct and just correct in format. The following step might take couple of minutes to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQM-tzXWUmoE",
    "outputId": "d125e6b1-f940-44b3-a05d-f837d2299f49",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# The evaluation might take up to couple of minutes to finish. Please be patient.\n",
    "\n",
    "(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n",
    "    test_dataset,\n",
    "    sampler,\n",
    "    **Configs.GENERATION_CONFIGS[\"greedy\"],\n",
    ")\n",
    "print(\n",
    "    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
    "    f\" {format_accuracy=}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CmB2ZT9Tn1l"
   },
   "source": [
    "# Train\n",
    "\n",
    "Let's set up all the configs first - checkpointing, metric logging and training.\n",
    "We then train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHzdsYsGTn1l",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Ckpt saving\n",
    "checkpointing_options = ocp.CheckpointManagerOptions(\n",
    "    save_interval_steps=Configs.SAVE_INTERVAL_STEPS, max_to_keep=Configs.MAX_TO_KEEP\n",
    ")\n",
    "\n",
    "# Metrics logger\n",
    "metrics_logging_options = metrics_logger.MetricsLoggerOptions(\n",
    "    log_dir=\"/tmp/content/tmp/tensorboard/grpo\", flush_every_n_steps=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWvBkWBsruom",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optimizer, learning rate scheduler, gradient clipping\n",
    "optimizer = optax.adamw(\n",
    "    learning_rate=optax.schedules.warmup_cosine_decay_schedule(\n",
    "        init_value=0.0,\n",
    "        peak_value=Configs.LEARNING_RATE,\n",
    "        warmup_steps=Configs.WARMUP_STEPS,\n",
    "        decay_steps=Configs.MAX_STEPS,\n",
    "        end_value=0.0,\n",
    "    ),\n",
    "    b1=Configs.B1,\n",
    "    b2=Configs.B2,\n",
    "    weight_decay=Configs.WEIGHT_DECAY,\n",
    ")\n",
    "if Configs.MAX_GRAD_NORM is not None:\n",
    "  optimizer = optax.chain(\n",
    "      optax.clip_by_global_norm(max_norm=Configs.MAX_GRAD_NORM),\n",
    "      optimizer,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T17:17:28.503426Z",
     "iopub.status.busy": "2025-12-02T17:17:28.503160Z",
     "iopub.status.idle": "2025-12-02T17:17:28.530369Z",
     "shell.execute_reply": "2025-12-02T17:17:28.529581Z",
     "shell.execute_reply.started": "2025-12-02T17:17:28.503407Z"
    },
    "id": "_6VxFW1ZTn1l",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training config\n",
    "cluster_config = rl_cluster_lib.ClusterConfig(\n",
    "    role_to_mesh={\n",
    "        rl_cluster_lib.Role.ACTOR: mesh,\n",
    "        rl_cluster_lib.Role.REFERENCE: mesh,\n",
    "        rl_cluster_lib.Role.ROLLOUT: mesh,\n",
    "    },\n",
    "    rollout_engine='vanilla',\n",
    "    offload_to_cpu=False,\n",
    "    training_config=rl_cluster_lib.RLTrainingConfig(\n",
    "        actor_optimizer=optimizer,\n",
    "        eval_every_n_steps=Configs.EVAL_EVERY_N_STEPS,\n",
    "        max_steps=Configs.MAX_STEPS,\n",
    "        mini_batch_size=Configs.TRAIN_MICRO_BATCH_SIZE,\n",
    "        train_micro_batch_size=Configs.TRAIN_MICRO_BATCH_SIZE,\n",
    "        # metrics logging\n",
    "        metrics_logging_options=metrics_logging_options,\n",
    "        # checkpoint saving\n",
    "        checkpoint_root_directory=Configs.CKPT_DIR,\n",
    "        checkpointing_options=checkpointing_options,\n",
    "    ),\n",
    "    rollout_config=base_rollout.RolloutConfig(\n",
    "        max_tokens_to_generate=Configs.TOTAL_GENERATION_STEPS,\n",
    "        max_prompt_length=Configs.MAX_PROMPT_LENGTH,\n",
    "        kv_cache_size=Configs.MAX_PROMPT_LENGTH + Configs.TOTAL_GENERATION_STEPS + 256,\n",
    "        temperature=Configs.TEMPERATURE,\n",
    "        top_p=Configs.TOP_P,\n",
    "        top_k=Configs.TOP_K,\n",
    "        eos_tokens=[1,106],\n",
    "    ),\n",
    ")\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    num_generations=Configs.NUM_GENERATIONS,\n",
    "    num_iterations=Configs.NUM_ITERATIONS,\n",
    "    beta=Configs.BETA,\n",
    "    epsilon=Configs.EPSILON,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4yJWiElSmOy"
   },
   "source": [
    "### Setting Up the GRPO Trainer\n",
    "\n",
    "Now we initialize our system for training. First, we create an `RLCluster` instance, which brings together the **policy model (`actor`)**, a **reference model (`reference`)**, and a **tokenizer**. Our `actor` is a trainable LoRA model, while the `reference` is a fixed base model that we use to guide the training.\n",
    "\n",
    "We then create a `GRPOLearner`, the specialized trainer that uses a list of **reward functions** to evaluate and optimize the model's output, completing the RL training setup.\n",
    "\n",
    "Tunix trainers are integrated with [Weights & Biases](https://wandb.ai/) to help you visualize the training progress. You can choose how you want to use it:\n",
    "\n",
    "**Option 1 (Type 1)**: If you're running a quick experiment or just testing things out, choose this. It creates a temporary, private dashboard right in your browser without requiring you to log in or create an account.\n",
    "\n",
    "**Option 2 (Type 2)**: If you have an existing W&B account and want to save your project's history to your personal dashboard, choose this. You'll be prompted to enter your API key or log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OIe1lO08Tn1l",
    "outputId": "017a2a3f-d9fd-4ac8-87f5-760e2586dfc9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# RL cluster\n",
    "rl_cluster = rl_cluster_lib.RLCluster(\n",
    "    actor=lora_policy,\n",
    "    reference=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    cluster_config=cluster_config,\n",
    ")\n",
    "\n",
    "# GRPO Trainer\n",
    "grpo_trainer = GRPOLearner(\n",
    "    rl_cluster=rl_cluster,\n",
    "    reward_fns=[\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    grpo_config=grpo_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8b71ed5"
   },
   "source": [
    "The first couple of training step might take up to 5 minutes to finish. Please be patient. If you experience long training steps, e.g. >10 minutes per step, please open a bug. Really appreciated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S27XDebYTn1l",
    "outputId": "2869f47f-c7d7-4bcc-b167-f05c77a619fa",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with mesh:\n",
    "  grpo_trainer.train(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzIP8glkTn1l"
   },
   "source": [
    "## Evaluate\n",
    "\n",
    "Let's evaluate our finetuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-73HfP1Tn1l",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load checkpoint first.\n",
    "import re\n",
    "\n",
    "# Find the latest checkpoint by listing directories in CKPT_DIR/actor\n",
    "actor_ckpt_dir = os.path.join(Configs.CKPT_DIR, \"actor\")\n",
    "\n",
    "latest_step = -1\n",
    "if os.path.exists(actor_ckpt_dir):\n",
    "  for item in os.listdir(actor_ckpt_dir):\n",
    "    if os.path.isdir(os.path.join(actor_ckpt_dir, item)) and re.match(r'^\\d+$', item):\n",
    "      step = int(item)\n",
    "      if step > latest_step:\n",
    "        latest_step = step\n",
    "\n",
    "if latest_step == -1:\n",
    "  raise FileNotFoundError(f\"No checkpoints found in {actor_ckpt_dir}\")\n",
    "\n",
    "print(f\"Latest checkpoint step: {latest_step}\")\n",
    "\n",
    "wandb.init(project='tunix-eval')  # logging bug workaround\n",
    "\n",
    "trained_ckpt_path = os.path.join(\n",
    "    Configs.CKPT_DIR, \"actor\", str(latest_step), \"model_params\"\n",
    ")\n",
    "\n",
    "abs_params = jax.tree.map(\n",
    "    lambda x: jax.ShapeDtypeStruct(x.shape, x.dtype),\n",
    "    nnx.state(lora_policy, nnx.LoRAParam),\n",
    ")\n",
    "checkpointer = ocp.StandardCheckpointer()\n",
    "trained_lora_params = checkpointer.restore(trained_ckpt_path, target=abs_params)\n",
    "\n",
    "nnx.update(\n",
    "    lora_policy,\n",
    "    jax.tree.map(\n",
    "        lambda a, b: b,\n",
    "        nnx.state(lora_policy, nnx.LoRAParam),\n",
    "        trained_lora_params,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vY9kl-ITn1l",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sampler = sampler_lib.Sampler(\n",
    "    transformer=lora_policy,\n",
    "    tokenizer=tokenizer,\n",
    "    cache_config=sampler_lib.CacheConfig(\n",
    "        cache_size=Configs.MAX_PROMPT_LENGTH + Configs.TOTAL_GENERATION_STEPS + 256,\n",
    "        num_layers=model_config.num_layers,\n",
    "        num_kv_heads=model_config.num_kv_heads,\n",
    "        head_dim=model_config.head_dim,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nz0q_gGHqYz6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# The evaluation might take up to couple of minutes to finish. Please be patient.\n",
    "(corr, total, accuracy, partial_accuracy, format_accuracy) = evaluate(\n",
    "    test_dataset,\n",
    "    sampler,\n",
    "    **Configs.GENERATION_CONFIGS[\"greedy\"],\n",
    ")\n",
    "print(\n",
    "    f\"{corr=}, {total=}, {accuracy=}%, {partial_accuracy=}%,\"\n",
    "    f\" {format_accuracy=}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1NMAxMh0H5D"
   },
   "source": [
    "With sufficient training, you should see that the percentages of correct model outputs have clearly gone up, which means our training worked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKd9Tp1d0dZS"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V6E1",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4054119,
     "isSourceIdPinned": false,
     "sourceId": 7045423,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31155,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
